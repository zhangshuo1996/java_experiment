
学校代码：  10286
分类号： TP311.52
密  级：  公 开
U D C：  004.4
学  号：  174374












一种用于描述学者画像的信息抽取系统
（学位论文形式：应用研究）

            研究生姓名： 李  微
            导师姓名： 李  微  副教授
                         梁学俊  高  工

申请学位类别      工程硕士                  	学位授予单位      东 南 大 学
工程领域名称      软件工程                	论文答辩日期   2020年   月   日
研究方向          软件工程               	学位授予日期   2020年   月   日
答辩委员会主席                  	评   阅   人

2020年   月   日



工程硕士学位论文



一种用于描述学者画像的信息抽取系统






专 业 名 称：  软件工程

研究生姓 名：   李  微

导 师 姓 名：   李  微

校 外 导 师：   梁学俊






AN INFORMATION EXTRACTION SYSTEM USED TO DESCRIBE SCHOLAR PORTRAITS

A Thesis Submitted to
Southeast University
For the Academic Degree of Master of Engineering

BY
Li Wei

Supervised by
Associate Prof. Li Wei
and
Senior Engineer Liang Xue-Jun

College of Software Engineering
Southeast University

July 2020

东南大学学位论文独创性声明

本人声明所呈交的学位论文是我个人在导师指导下进行的研究工作及取得的研究成果。尽我所知，除了文中特别加以标注和致谢的地方外，论文中不包含其他人已经发表或撰写过的研究成果，也不包含为获得东南大学或其它教育机构的学位或证书而使用过的材料。与我一同工作的同志对本研究所做的任何贡献均已在论文中作了明确的说明并表示了谢意。

研究生签名：               日期：


东南大学学位论文使用授权声明

东南大学、中国科学技术信息研究所、国家图书馆、《中国学术期刊（光盘版）》电子杂志社有限公司、万方数据电子出版社、北京万方数据股份有限公司有权保留本人所送交学位论文的复印件和电子文档，可以采用影印、缩印或其他复制手段保存论文。本人电子文档的内容和纸质论文的内容相一致。除在保密期内的保密论文外，允许论文被查阅和借阅，可以公布（包括以电子信息形式刊登）论文的全部内容或中、英文摘要等部分内容。论文的公布（包括以电子信息形式刊登）授权东南大学研究生院办理。

研究生签名：         导师签名：         日期：


摘要
产学研合作促进了科技和产业发展，但在推进产学研合作的过程中，企业或者地区政府难以获取学者的相关信息，不能高效地对话。主要原因是领域学者信息分散，信息获取的难度大。互联网中存在着描述学者信息的数据，可以通过数据采集和信息抽取，聚合和处理这类数据，形成精简的学者描述信息，为企业和地区政府提供帮助。
本文设计并实现了一种用于描述学者画像的信息抽取系统，目的在于将互联网中的数据聚合成描述学者的信息，给数据平台提供数据支撑，展示给企业或政府，让其对学者有一定的了解。与市面上已有处理学者数据的软件系统不同的是，本系统更关注在产学研背景下学者的相关信息，例如学者的年龄、教育经历以及社会关系等信息。本文的主要工作如下所示：
（1）提出了学者主页发现和抽取的方法。提出了基于搜索引擎搜索和识别出主页，得到主页数据，在保证准确率的条件下，提升了数据收集的效率。同时提出了基于文本分句和实体识别技术的学者画像抽取方法。
（2）提出了基于学术文献数据的学科领域术语抽取。采用C-Value和互信息为重要抽取指标，对学术文献数据中的词语进行指标计算。为了扩展新术语，本文还提出了一种词组的词性规则自动生成的算法，提升了术语的抽取率。
（3）设计并实现了一种面向学者画像的信息抽取系统。通过分析产学研背景下的学者画像模型，定义抽取目标范围，设计抽取策略，实现了信息抽取系统。系统能够有效地完成主页抽取和术语抽取任务，抽取的结果能够被三螺旋数据平台直接利用，验证了系统的可用性与有效性。

关键词：学者画像，信息抽取，实体识别，C-Value
 
Abstract
Industry-university-research cooperation has promoted the development of science and technology and industry, but in the process of advancing industry-university-research cooperation, it is difficult for enterprises or regions to obtain relevant information from scholars and cannot conduct effective dialogues. The main reason is that the information of scholars in the field is scattered and it is difficult to obtain information. There are data describing scholars' information on the Internet. This type of data can be aggregated and processed through data collection and information extraction to form streamlined scholars' description information and provide assistance to enterprises and regional governments.
This paper designs and implements an information extraction system for describing scholars’ portraits. The purpose is to aggregate data from the Internet into information describing scholars, provide data support to the data platform, and display it to enterprises or governments, so that they can be certain to scholars. Understanding. Different from the existing software systems on the market that deal with scholar data, this system pays more attention to scholars' related information in the background of industry, education and research, such as scholars' age, educational experience, and social relations. The main work of this paper is as follows:
(1) The method of discovering and extracting scholar homepages is proposed. It is proposed to search and identify the homepage based on a search engine, and obtain the homepage data. Under the condition of ensuring the accuracy, the efficiency of data collection is improved. At the same time, a scholar portrait extraction method based on text segmentation and entity recognition technology is proposed.
(2) Proposed the subject field term extraction based on academic literature data. C-Value and mutual information are used as important extraction indexes to calculate indexes for words in academic literature data. In order to expand the new terminology, this paper also proposes an algorithm for automatically generating the part-of-speech rules of phrases, which improves the extraction rate of terms.
(3) Designed and implemented an information extraction system for scholar portraits. By analyzing the portrait model of scholars under the background of industry-university-research research, defining the extraction target range, designing extraction strategies, and realizing the information extraction system. The system can effectively complete the homepage extraction and term extraction tasks, and the extracted results can be directly used by the triple helix data platform, which verifies the usability and effectiveness of the system.

Keywords: Scholar Portrait; Information Extraction; Entity Recognition; C-Value

目 录
摘要	I
Abstract	II
第一章 绪论	1
1.1 研究背景及意义	1
1.2 国内外研究现状	1
1.2.1 学者画像	1
1.2.2 信息抽取	2
1.3 主要研究内容	4
1.4 论文结构	4
第二章 相关研究基础	6
2.1 学者画像模型分析	6
2.2 文本特征提取	6
2.2.1 TF-IDF	7
2.2.2 Word2Vec	7
2.2.3 BERT	8
2.3 文本分类模型	9
2.4 术语抽取	10
2.4.1 语言学特征判别	10
2.4.2 术语特征	10
2.4.3 C-Value特征	11
2.4.4 互信息特征	11
2.5 评价方法	11
2.6 本章小结	12
第三章 需求分析	13
3.1 业务分析	13
3.2 功能性需求分析	14
3.2.1 信息抽取系统功能性需求描述	14
3.2.2 用户角色与系统接口的确定	14
3.2.3 信息抽取系统具体功能分析	15
3.3 非功能性需求分析	16
3.4 本章小结	17
第四章 信息抽取系统设计	18
4.1 系统总体设计	18
4.1.1 三螺旋数据服务平台简介	18
4.1.2 信息抽取系统总体框架	19
4.1.3 信息抽取系统部署框架	20
4.2 系统详细设计	20
4.2.1 系统功能概述	20
4.2.2 数据获取与预处理模块设计	21
4.2.3 主页抽取模块设计	24
4.2.4 术语抽取模块设计	32
4.2.5 应用服务模块设计	34
4.3 数据存储设计	38
4.4 本章小结	42
第五章 信息抽取系统实现	43
5.1 系统开发环境	43
5.2 数据获取与预处理模块实现	43
5.2.1 采集器	43
5.2.2 主页链接识别	44
5.2.3 预处理	45
5.3 主页抽取模块实现	45
5.3.1 文本分句	45
5.3.2 分词与实体识别	46
5.3.3 文本单元分类	49
5.3.4 冗余信息去除和数据集成	50
5.4 术语抽取模块实现	51
5.4.1 数据接口	51
5.4.2 术语抽取器	51
5.4.3 主程序控制	54
5.5 应用服务模块实现	55
5.5.1 模块项目结构介绍	55
5.5.2 请求参数处理	56
5.5.3 返回消息处理	56
5.6 本章小结	57
第六章 系统测试与性能分析	58
6.1 单元测试	58
6.1.1 数据获取与预处理模块	58
6.1.2 主页抽取模块	59
6.1.3 术语抽取模块	61
6.1.4 应用服务模块	62
6.2 系统功能测试	63
6.2.1 数据接口功能测试	63
6.2.2 后台管理功能测试	64
6.3 系统评测	66
6.3.1 数据准备	67
6.3.2 主页链接识别	67
6.3.3 主页抽取	68
6.3.4 术语抽取	71
6.4 本章小结	72
第七章 总结与展望	73
7.1 总结	73
7.2 展望	73
参考文献	75

第一章 绪论
1.1 研究背景及意义
本课题来源于企业横向课题《三螺旋数据公共服务平台》。该平台依靠收集的互联网中的海量数据，挖掘整理出专家、专家团队相关资料和全国科研资源现状，同时维护企业与地区相关资源现状，按需为双方牵线搭桥，促成合作。同时，也为政府促进区域创新与技术转型升级提供服务。
平台将大学、产业、政府三方需求有机结合起来，意在解决目前大学、产业、政府之间的互动合作中存在着一些亟待解决的问题，例如：（1）地区政府或者企业与高校的合作高度依赖领域相关人员的介绍和推荐，使得这个过程受到地域和人脉的限制；（2）理论研究与相关产业的语境不完全相同，企业或政府在引进技术解决企业技术需求时，对相关领域研究现状缺乏了解；（3）各领域学者信息分散，使得学者信息获取的难度加大，使得交流沟通不能有效建立。想要提高产学研合作的高效性，就要求平台能够快速匹配专家学者团队、定位研究领域并取得联系以及后续合作。
本次课题定位为平台的一个子系统，旨在解决专家学者信息获取、整合的问题。随着科学技术的发展，开放互联网中的学术数据，包括学者主页、学术论文、项目信息等，变得越来越丰富。海量的数据蕴含着丰富的信息，这为挖掘专家学者、刻画学者画像提供了数据基础。纵观整个互联网，已存在的学术数据管理平台可以分为两种，一种是面向文档信息的平台，例如百度学术、谷歌学术、Citeseer以及DBLP等，这些平台将论文、专利等文档的结构化组织作为其主要的研究对象；另一种是面向学者的，例如，Aminer[1]、Acemap[2]等，这些平台以学者为中心，结构化组织学术数据，致力于学术与前沿技术的挖掘与分析。
本系统采用以学者为中心的研究方法，更加注重在产学研背景下的应用。因此，如何定义定在产学研背景下学者画像和确定相关数据的范围成为本课题研究的首要工作，如何克服海量数据带来的噪音冗余问题，以及如何从噪声数据中获取有效的信息是本课题需要克服的难点。
学者画像的内涵丰富，其完整分析过程主要包括三个方面：学者信息标注、研究兴趣挖掘、学术影响力预测。其中，学者画像的基本信息、联系信息、教育经历等一系列具体信息，易于从互联网中的数据抽取获得，形成相应的学者基本属性文档，为之后的学者画像研究提供基础数据，也为整个平台的搭建提供数据支撑。
1.2 国内外研究现状
1.2.1 学者画像
用户画像的概念最早由交互设计之父A.Cooper提出，他将用户画像定义为“用户真实数据的虚拟代表”。随着互联网行业的蓬勃发展，为解决产品运营中的用户定位不精准、用户运营中的个性化服务不足问题，将用户画像引入用户行为分析，通过对用户打标签、建立数据模型来解读“全样本”用户的行为特征开始成为互联网产品设计及运营的趋势。例如，在电子商务系统中，用户的历史购物习惯和偏好对商品的定向推荐和营销有着极其重要的作用；在社交网络中，用户的个人信息和社交交互数据能被用于好友推荐和社群发现[3]。
在互联网应用领域，用户画像主要指以真实用户群体为对象[4][5]，以用户的静态属性(人口统计特征、空间和地理特征等)和动态属性(兴趣爱好、行为模式等)数据为基础，通过定性或定量方法提炼抽象出的具有显著特征的用户模型。虽然在不同的应用中，实现用户画像的具体参数有所不同，但是实现用户画像的技术具有通用性。
传统的用户画像大多通过问卷调查、用户访谈等方式[6]，分析出用户的共性与差异，形成不同的用户画像。依靠人工和用户配合的方式往往需要花费大量时间和资源，对人工资源要求高的同时，数据获取的效率极其低下。同时，也会存在用户配合度不高，使得获取的信息的完整度和一致性不满足用户画像模型的问题。
近年来，利用计算机技术自动构建用户画像的方式逐渐取代手工，成为主流。这类方法首先开放互联网中这个庞大资源库中收集大量电子文档，然后用预定义的规则或模型抽取各项信息。为了表达的准确与清晰，将面向科研学者的用户画像技术称为“学者画像”。袁莎等人[7]认为面向科研学者的用户画像构建有三项基本任务：学者信息标注、研究兴趣挖掘以及未来影响力预测，使得画像构建任务分解为多个可以用计算机自动完成的任务。
1.2.2 信息抽取
信息抽取理论是计算机自动构建用户画像的基础理论。关于信息抽取的研究可以追溯到上世纪60年代，以美国纽约大学的Linguistic String[8]和耶鲁大学的FRUMP[9]这两个长期项目为代表。80年代消息理解会议（MUC，Message Understanding Conference）的召开推动了信息抽取的研究与应用，该会议上制定了一套基于模板填充的信息抽取方案，用于处理命名实体识别、关系抽取、事件抽取、共指消解等具体内容。随着会议的逐年举办，出现了很多技术和方法，如PALKA[10]，AutoSlog[11]， CRYSTAL[12]等。继MUC会议之后，自动内容抽取（Automatic Content Extarction，ACE）评测会议、多语种实体评价任务会议（Multilingual Entity Task Evaluation，MET）、文本理解会议（Document Understanding Conference，DUC）等与信息抽取相关的学术会议为信息抽取在不同领域、不同语言中的应用起到了很大的推动作用。
目前，信息抽取系统的具体实现方法可以分为基于规则模式的方法[13][14]和基于统计学习[15][16]的方法。基于规则的方法具有较高的效率与准确率，早期的规则生成依靠某个领域的专家知识，构造大量模板，时间效率与人力成本上代价较大。但是基于规则的方法有其自身的局限性，如：人工编制规则的过程较复杂、通过机器学习得到的规则效率较低、系统通用性差等。后来的研究逐渐转向基于统计的方法，例如，俞鸿魁、张华平等人提出了一种基于层叠HMM模型的中文命名实体识别方法，成为中文词法分析工具ICTCLAS的核心实现算法[8][9]。基于统计的信息抽取，虽然可以从一定程度上弥补基于规则方法的缺陷，但是随着研究的深入，人们发现基于统计的方法并不是完美的，所以现在的研究又开始考虑采用将基于规则和基于统计的方法相结合的策略来寻找效果更佳的信息抽取方案。如，AlanRitter等人[10]提出一种基于文本分类和词性标注相结合的方法处理微博文本；鞠久朋等人[17]提出一种CRF与知识规则相结合的地理空间命名实体识别方法，F测试值达到91.87%。
按照研究对象，信息抽取技术的研究大致可以分为两大类，一类是结构化文本抽取信息，另一类是web信息抽取。学者画像的自动化构建并是依赖于对互联网中的web信息进行信息抽取，得到描述学者画像的目标属性。对于web信息抽取，Hsu[27]认为web文本的结构化程度取决于用户想要抽取的信息属性，当抽取的信息存在于网页段落打断的自由文本段落中时，属于传统的信息抽取范畴；而当抽取的信息存在于表格、列表等段落中时，属于半结构化文本信息抽取。
Web信息抽取，对HTML结构的网页进行抽取时，大多先提取出网页中的正文部分，然后采用实体识别或者文本分类方法获得结果。Gupta[18]、孙承杰[19]等人通过统计各个标签中链接文本与普通文本的比值的方法，定位网页正文区域。微软亚洲研究院的Cai[22]引入网页的视觉特征来抽取信息，如VIPS算法，但在不规范页面中效果较差。而另一种有效的方法是基于DOM树[23]，将HTML页面表达为DOM树的结构，进行相关计算，分离出网页正文，其代表性的算法有DSE[24]、MDR[25]等。
术语抽取是信息抽取领域中一个基础而又重要的研究方向，可以应用在本体构建[29]、机器翻译[30]和语义检索[31]等诸多研究领域。早期的术语自动抽取中使用的大多是基于语言学知识，通过总结一些词性组成规则提取术语。随着自然语言处理技术的发展，术语抽取系统中逐步引入了一种或多种统计策略，例如领域相关性[32]、互信息（Mutual Information）[33]、对数似然比（Log-Likehood Rato）[34]等。也有基于词性规则提出的统计指标C-Value[35]，以及将关键字抽取与C-Value结合抽取术语的方法[36]。随着机器学习算法的在词性标注和命名实体识别领域的使用，该类方法也逐渐被引入到术语的抽取研究中，例如使用CRFs模型进行术语抽取[38][39]，以词语、词性以及词语的TF-IDF值作为特征进行术语抽取，取得了不错的成果。
现阶段，对学术信息进行分析的代表性的信息抽取系统有Google Facts[26]、Aminer、Acemap等。其中Google Facts 则通过海量数据进行相互佐证，分析整理互联网上的知识。Aminer、Acemap，它更为关注学者的学术能力，以论文成果为核心构建学者的社交体系，对学者画像的刻画还存在不足。
本文采用以学者为中心的研究方法，更注重对学者画像的描述和在产学研背景下的应用。首先，需要确定学者画像模型和抽取范围；然后，使用信息抽取的技术从非结构化的数据中抽取想要数据，填充至学者画像模板。
1.3 主要研究内容
本文针对产学研背景下的学者画像应用问题，实现用于描述学者画像的信息抽取系统，主要研究内容如下所示：
（1）数据采集与预处理
学者画像的描述信息主要来源于对学者主页和发表的学术文献，需要进行数据采集与预处理工作。其中，学术文献数据（主要包括专利、论文）由平台提供，对直接从平台数据库中拿到的数据需要进行去重、去除无效数据等处理；主页数据需要通过网络爬虫获取，并对对其进行预处理，为后续抽取提供数据基础。
（2）主页信息抽取
对学者主页，需要根据应用背景和主页自身的特征设计抽取重要信息的策略。首先，使用基于HTML结构特征的分块技术对主页进行分块；然后，对每一个文本块提取实体特征和语义特征后，进行文本块分类；对不同类别的文本块提取最终结果中需要的实体名词和专有名词，从而得到存在于学者主页中的学者画像相关属性。
（3）术语抽取
术语抽取主要是对学者的文献数据进行处理，得到便于描述学者研究方向的领域名词。首先，以学科为单位构建了种子术语词典库；然后，对文献数据中的词语采用词性组合和指标计算结合的方式，拓展新的术语，最后得到领域内术语权重的排序列表。
（4）应用服务接口
应用服务接口是指系统对数据的管理和使用的能力。首先，通过资源管理风格的接口设计，将数据与抽取资源化；然后，实现简单的交互功能，给用户提供访问和管理资源的入口。
1.4 论文结构
本文分为七章，每章的具体内容如下。
第一章绪论，介绍了课题的研究背景与意义，阐述了学者画像与信息抽取技术的国内外研究现状，介绍了论文的研究内容与组织结构。
第二章相关研究基础，首先介绍了学者画像模型，分析了产学研背景下学者画像的内涵，然后介绍了本文实现用于描述学者画像的信息抽取系统所用到的相关技术。
第三章系统需求分析，首先分析了产学研平台的业务，根据产学研业务分析了系统的功能性需求，最后阐述了系统的非功能性需求。
第四章系统设计，首先介绍了系统的总体设计，包括三螺旋数据服务平台的见解、信息抽取系统的总体框架与部署框架，然后针对系统的功能需求，进行了系统的详细设计，最后对系统中涉及到的数据进行存储设计。
第五章系统实现，介绍了系统的具体实现过程、主要代码等。
第六章系统测试与性能分析，首先介绍了测试环境，然后对各个功能模块进行了单元测试，对整个系统进行功能测试，最后对系统所用的方法进行了分析与评价。
第七章总结与展望，对本文所做的工作进行了总结，指出了本文所述系统存在的不足与改进的方向。
 
第二章 相关研究基础
2.1 学者画像模型分析
本系统主要是用于面向学者画像对互联网中的学者数据进行抽取整合，得到部分学者画像数据，为了确定学者数据范围，首先需要对产学研背景中的学者画像进行分析。
《三螺旋数据服务平台》中学者画像的数据模式如图 2-1所示。从产学研社交出发，将学者画像分为两个主要部分：个人属性与社会属性。个人属性包括姓名、性别、出生年月等基本信息，也包括工作信息与学历信息；社会信息包括联系信息（通讯地址、邮箱、电话、微信等）、行政头衔（教授、副教授、研究员等）、荣誉信息（院士、长江学者、杰出青年、国家奖项等）、学术信息（项目、研究成果、研究兴趣）等静态信息，也包括社交活动信息（拜访信息、活动资料、合同信息），社交网络信息（ego-net、现实学者社区等）动态信息。
对于学者画像而言，基本信息、工作信息、学历信息、联系信息以及行政头衔、部分荣誉信息等可以通过采集互联网中的数据，再利用信息抽取技术得到；其他部分信息可以通过整合在线数字图书馆，或进一步挖掘得到，如学术信息、社交网络信息；也有部分信息通过线下运营活动产生，如社交活动信息。

图 2-1学者画像模型
2.2 文本特征提取
为了使用机器学习算法来对文本进行分析，需要把文本被转换成机器可以识别的数字向量形式，作为输入。词向量自然语言处理任务中文本向量化的重要技术，常应用于在标注、文本分类、情感分析、问答系统等任务中。词嵌入过程可以把一个维数为所有词数量的高维空间嵌入到一个维数低得多的连续向量空间中，生成词向量。常见的词向量编码有One-Hot、TF-IDF、Word2Vec等。One-Hot编码向量通过词汇大小的向量表示文本中的词，不能很好的表示词与词之间的隐含关系，是最基础的词向量。一个很好的编码思想是将单词从原先所属的空间映射到新的多维空间中，词义相近的词在多维空间中的距离也相近。当前主流的词向量模型有WORD2VEC和BERT便是采用这一思想。
2.2.1 TF-IDF
TF-IDF是一种统计方法，用于评估某个词语在文件集或者语料库的某一文档中的重要程度。TF-IDF同时考虑到词语的词频和文档频率，词语的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降，其具体计算如式(2.1)所示。

(2.1)

其中， 词频， 表示词语 在文本 中的频数， 表示文档 中所有词汇量总和。 表示语料库中所有词语的频数之和， 表示词语 在语料库中的总频数。
TF-IDF虽然简单易行，在此分类任务中也有较好的效果，但也存在一定的缺点：1）TF-IDF没有考虑特征词的位置对词重要程度的影响，缺乏语义信息；2）传统TF-IDF函数中的逆文档率只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况，因此某些生僻的非特征词也会提取出来。
2.2.2 Word2Vec
语义是自然语言处理过程中的重要特征，在提取语义特征的时候，通常的一个思路就是通过概率表征，将其映射到向量空间中，这样相近含义的文本会在向量空间中有较近的距离。Word2Vec就是将文本映射到语义空间中，运用深度学习算法，将词语表示成高密度低维向量。
Word2Vec模型的结构分为接受的是词语one-hot向量的输入层，进行向量相加或恒等投射的投影层和输出结果的输出层。Word2Vec的具体算法包括CBOW和Skip-Gram两种。CBOW算法的思路运用上下文预测词语，其思想可以用公式(2.2)概括。其中， 为训练语料中的词语对应的向量， 表示算法内对上下文中一定窗口内词向量的相加运算， 为算法中窗口大小的一半。CBOW算法的具体结构如图 2-2所示。Skip-Gram算法的思路是基于中心词预测上下文的词语，其数学表示如公式(2.3)所示，其具体架构如图 2-3所示。
CBOW模型和Skip-Gram模型除了输入层和输出层的数据含义不一样，在投影层的计算也有所不同，CBOW模型的投影层对输入的词语向量进行了相加计算，而Skip-Gram模型的投影层进行的是恒等投射的操作，其目的是与CBOW结构保持一致。除了结构上的区别，Skip-gram与CBOW相比有更高的准确度，但前者复杂度高出后者，训练更加耗时。这主要是因为CBOW模型利用了滑动窗口，忽略了窗口之外的词语；而 Skip-Gram算法没有窗口的限制。因此在语料过大时，通常选用CBOW算法来提高训练效率；在语料较小时，选用Skip-Gram，能得到更高的准确度。

(2.2)

(2.3)

图 2-2 CBOW模型

图 2-3 Skip-Gram模型
2.2.3 BERT
BERT（Bidirectional Encoder Representations from Transformers）是一种预训练语言表示的方法，其在大量文本语料（维基百科）上训练了一个通用的“语言理解”模型。模型作为预训练模型，可以方便的执行下游NLP任务。在Google AI Language发表的论文中标明BERT模型在11个NLP任务上的表现取得了新的好的结果。
BERT的结构如图 2-4所示，其中模型输入中Token Embedding是通过查询字向量表将文本中的每个字转换为一维向量，Segment Embedding是指在做两个序列任务（比如Q&A）的时候对不同序列做标识，Position Embedding是对文本中不同位置的字或词分别附加一个不同的向量，模型输出则是融合全文语义信息后的最终向量表示。与传统静态词向量不同，动态词向量可以根据具体的上下文信息，捕捉文本中的双向关系，动态生成词向量。Google开源了BERT模型，用户可以直接使用BERT作为自己任务中的向量输入。

图 2-4 BERT模型结构
2.3 文本分类模型
Boosting方法训练基分类器时采用串行的方式，将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，最终结果为各层分类器的加权叠加的结果。常见的Boosting方法模型有AdaBoost、GBDT以及XGBoost。
XGBoost是GBDT的变种（Greedy Function Approximation：A Gradient Boosting Machine），在速度和效率发挥到极致。相比于GBDT，XGBoost的主要优点在于：（1）GBDT以CART作为基分类器，XGBoost支持线性分类器和CART分类器；（2）GBDT用到一阶导数信息，XGBoost对代价函数进行了二阶泰勒展开，同时用到一阶与二阶导数，支持自定义代价函数；（3）XGBoost在代价函数中加入正则化项，防止过拟合；（4）XGBoost可以自动学习缺失值的分裂方向；（5）XGBoost支持并行训练，训练速度快于GBDT。
XGBoost的目标函数见式(2.4)，其中 为损失函数， 为正则项， 为常数项。对于每一个 ，利用泰勒展开三项，近似计算目标函数，具体计算如式(2.5)、(2.6)、(2.7)所示。

(2.4)

(2.5)

(2.6)

(2.7)
总得来说，XGBoost的训练过程就是不断地进行特征分裂来生长一棵树，学习一个新函数 ，去拟合前面一次预测的残差，直到训练完成，得到K棵树；在预测时，样本的特征会对应到每棵树的叶子节点，每个叶子节点对应一个分数,K棵树对应的分数加起来便是该样本的预测值。
2.4 术语抽取
2.4.1 语言学特征判别
术语按其组成长度可划分为单词短术语和多词长术语，单词短术语由单个词汇构成，下文简称短术语，多词长术语一般由2到6个词组成。作为某个领域反复使用、形势较为固定又 表达某特定概念的词语，术语的组成结构一般具有词性特点。多词型长术语的词性规则比较复杂，文献[37]总结了2到6词的术语构词规则，如表 2-1所示。
表 2-1 多词组合词性规则
规则	说明
N+N	名词与名词的二词组合
(A|N)+N	形容词或名词与名词的二词组合
((A|N)+|((A|N) * (NP)?(A | N) * )N	多词组合
2.4.2 术语特征
术语是指某一专业领域的有限定专业概念的特殊词语，它是专业领域知识系统中的重要组成部分，在专业领域文本中有着重要的表征作用。术语抽取工作往往基于两个重要的特性：单元性，指一个词或词组是否可以表达一个独立、完整的语言含义，并具有稳定的结构；领域性，指一个词或词组与特定领域的相关性，用于度量该词或词组对领域知识的表达能力。
当前的术语抽取技术主要有三类：基于规则、基于统计、有监督的学习算法。在没有标注好的训练集的情况下，将规则与统计结合，建立一个复合标准往往会取得较好的效果。
2.4.3 C-Value特征
C-Value的计算考虑到了简单术语与复杂术语之间的关系，通过定义候选短术语和候选长术语之间的关系，提升复杂术语的抽取率。若定义 不被任何其他串包含的字符串，则称 为“极大串”；字符串 包含字符串 ，则 为 的“母串”， 为 的“子串”。根据一个串是否嵌套在其他串中，可分为式(2.8)和(2.9)两种情况：
 ，当 为极大串时
(2.8)
 ，当 为嵌套串时
(2.9)
其中， 表示 的长度， 表示 在语料中出现的次数， 表示包含 的母串， 表示 在语料中出现的次数， 表示包含 的母串的集合， 表示集合 的大小。
2.4.4 互信息特征
互信息是指两个时间集合之间的相关性，本系统采用互信息度量词语的单元性。词组 的互信息计算公式如式(2.10)所示。

(2.10)

(2.11)
其中， 表示概率，当语料足够充分时，公式中的概率可以用频率替代，在术语抽取中 和 分别表示词组 的最长前缀和最长后缀。
Magerman和Marcus提出了广义互信息的概念，将两个元素之间的相关扩展到N个元素之间，具体公式如式(2.11)所示。其中，K为元素之间的分割点， 表示N个元素间任意两个部分之间的互信息；表示这个向量中的第K个分量，表示 和 之间的互信息，其中局部广义互信息值最小值应为短语的边界。
2.5 评价方法
评价抽取任务的指标主要有准确率(Precision)和召回率（Recall）。定义N为页面中所包含的全部该字段信息的个数，M 为抽取得到的结果中的个数，Acc 为抽取结果中正确的个数，则准确率的公式为式(2.11)，召回率的公式为式(2.12)。准确率表达了系统希望能够尽可能的提高识别得到的数据的准确程度，召回率则是期望能够尽可能全的找到所有应该识别的信息。

(2.11)

(2.12)
对排序类型的结果采用mAP的评价方式，其中AP指的是不同召回率上的正确率的平均值，mAP即为m个类别AP的平均值。mAP的具体计算流程如下所示：
1、对于某个类别，将算法输出的所有该类别的预测框按某一排序指标排序；
2、设定多个大小为K的预测框，选取Top K个结果，统计其中的FP（实际为负例，预测为正例）和TP（实际是正例，预测为正例）的个数，使得召回率分别等于0，0.1，0.2，0.3，0.4，0.5，0.6，0.7，0.8，0.9，1.0，计算其准确率Precision；
3、计算AP值，即Average Precision，为步骤2得到的11个准确率的平均值；
4、重复步骤1~步骤3，重复计算其他类别的AP值；
5、计算mAP值，mAP为所有类别的AP之和除以类别数。
2.6 本章小结
本章主要介绍了本文的研究对象和关键技术。首先，介绍了产学研背景下的学者画像模型的定义；然后，介绍了用于学者画像的信息抽取系统所涉及的关键技术，包括文本特征提取、文本分类算法和术语抽取相关算法。为本文所实现的信息抽取系统奠定了理论基础。
 
第三章 需求分析
3.1 业务分析
在三螺旋平台实际服务产学研活动的过程中，高校的运营人员通过平台聚合的高校学者及学者团队相关的数据，了解相关高校的学者团队情况，定位合适的学者团队，再与团队中的某些学者取得联系，形成平台与高校的社交关系网。企业的运营人员亦是如此，最终形成平台-企业、平台-高校的社交网络。平台所聚合的学者团队与学者的信息是与数据部门的工作分不开的，例如：学者、学者团队及企业相关数据的收集与整合、学者团队的分析等。其中，一部分数据便是学者画像，即包含了学者个人属性与社会属性的学者档案。
学者画像的研究有三个部分：1）建立学者画像标签体系。通过数据部门与运营部门的多次沟通，确定适当的学者画像模型，建立标签体系。2）数据采集，标签映射。收集与分析用户社会属性、行为习惯等主要信息后，抽取用户信息并进行标签化和结构化处理。3）进行数学建模。从已有的数据中挖掘出更深层次的用户潜在信息，并通过数据可视化技术为用户展示有价值的信息。
本系统以“三螺旋大数据服务平台”的数据部门为目标组织，对开放互联网中的学者数据进行自动化的收集与抽取，以求解决学者数据多源、手工收集效率低、数据形式非结构化的问题，不仅为学者画像的研究提供部分数据，也为平台的搭建给与数据支持。图 3-1为本系统抽取的学者画像标签，是三螺旋平台定义的学者画像标签体系的子集。
本系统具体的应用场景应该有以下两种：
（1）为数据部门研究人员提供所需要的学者画像研究的部分数据，如学者的个人基本信息、学历信息、荣誉信息，以及与学者研究方向有关的领域术语列表；
（2）为平台其他子系统提供需要的相关数据，例如搜索子系统的构建需要术语数据、平台中最终学者个人信息的展示等。

图 3-1信息抽取系统学者画像标签
3.2 功能性需求分析
3.2.1 信息抽取系统功能性需求描述
（1）抽取目标范围。通过对学者画像标签的建立，了解到不同维度标签的具体取值需要从不同来源、不同形式的源数据中抽取获得。具体来说，部分数据来自学者主页数据，这些主页数据包括学院官网学者主页、百度百科个人信息页、中国工程院院士介绍页等，先对这部分数据进行收集，再进行抽取和存储；另一部分术语数据则需要从学者论文和专利中得到，需要能够对论文与专利文本进行领域术语的抽取。对抽取的数据进行整合以及存储。
（2）抽取任务管理。系统应满足用户通过图形化界面的操作，对学者画像相关数据信息抽取任务的进行管理，例如任务的配置和操作。信息抽取系统需要将每种类型的抽取任务流程化，流程中包括主页页面数据源下载和论文数据获取、主页抽取与术语抽取、存储信息抽取结果数据这几部分。流程化的抽取过程使得系统能够接受用户配置的参数，完成抽取工作。
（3）系统对外接口。根据业务分析可知，不用的应用场景对信息抽取系统如何提供抽取服务有不同的要求。一方面，用户期望可以通过一个用户界面来配置和操作抽取任务息抽取任务。信息抽取系统应当可以让用户通过配置抽取任务参数，并执行相应的抽取工作；在抽取任务完成之后，将抽取的结果以结构化的形式存储，并为用户提供浏览、修改、删除的功能。另一方面，用户也需要面向学者画像的信息抽取系统能够作为一个信息抽取平台，提供一系列Http形式的服务接口，供外部程序调用。用户可以以输入参数的方式指定信息抽取目标等配置信息，通过调用接口，得到系统返回的结构化的抽取结果。
3.2.2 用户角色与系统接口的确定
根据业务分析及抽取系统对外接口功能描述可以确定本系统的用户角色主要有两类。一类用户角色的典型代表是学者画像研究人员或系统管理员，他们需要通过可视化的用户界面，配置抽取任务参数，执行信息抽取工作，并可以查看和导出抽取的结果，或对抽取结果进行修改；另一类用户的典型代表是三螺旋数据服务平台其它子系统，例如搜索子系统、学者画像子系统、平台网站等，他们需要在信息抽取系统抽取的结果数据的基础上，选取自己需要的数据应用在该系统上。例如，搜索子系统需要的领域术语集，学者画像子系统需要的通过抽取获得的学者数据，平台网站显示学者主页数据需要聚合抽取的学者画像数据、社交网络数据以及其他的学术相关数据。
图 3-2是整个系统的总体用例视图。这个用例视图描述了使用本系统的两类用户角色，以及系统两种不同的对外服务接口。

图 3-2系统总体用例视图
3.2.3 信息抽取系统具体功能分析
（1）数据获取功能。根据系统的业务分析可知，本系统的数据来源为开放互联网中的学者主页数据和论文数据。对于学者论文数据，通过接入命名消歧子系统取得对应学者的论文数据即可。对于学者主页数据，则需要本系统直接从开放互联网中取得。所以，本系统应当具有从互联网中收集数据链接，并获取需要的数据的功能。
（2）数据预处理功能。直接获取的数据往往存在着异构、稀疏、缺失的问题，为了提高信息抽取的准确率，系统需要有数据预处理功能，使得后续的抽取步骤能够得到干净的数据。具体包括，统一编码、繁简转化、冗余去除等过程。
（3）主页抽取功能。主页抽取功能是指能够对前面步骤中收集到的主页数据进行抽取，得到学者画像标签数据模式中的部分数据。包括，从主页数据中抽取出学者的个人基本信息（姓名、年龄、职称）、学历信息、荣誉信息，并存储。
（4）术语抽取功能。术语抽取功能是指能够对学者相关的论文专利数据进行领域术语抽取，并给以术语词频统计。
（5）应用管理功能。本系统提供的应用管理功能是基于抽取任务和结果数据的用户界面，针对的用户角色有学者画像研究人员或系统管理员。这部分的具体功能包括：通过信息抽取系统图形化用户界面配置信息抽取任务，包括：定义抽取任务的范围；控制信息抽取任务的执行；可以浏览或导出信息抽取结果；同时能够手动修改抽取结果并更新到数据库中。详细的用例图如图 3-3所示。
（6）抽取结果数据接口。针对使用数据服务接口的用户角色，例如搜索子系统、学者画像子系统、平台网站等，可以使用通过配置参数发送Http请求的方式，定义目标数据范围和返回数据格式，以获取合适的结果数据用于该系统。图 3-4为Http服务接口用例视图，从使用Http服务接口的用户角色的出发，详细定义了系统的对外接口功能。

图 3-3 用户界面用例视图


图 3-4 Http服务接口用例视图
3.3 非功能性需求分析
除了功能性需求外，系统还应当满足以下非功能性需求：
（1）易用性需求。易用性会使产品提高符合用户习惯的能力以及其对使用的期望，本系统要保证整个系统涉及的概念易懂，操作简单，使得用户能够快速熟悉本系统。
（2）可执行需求。可执行性是指产品可以在给定的时间或者特定的精确度来执行某些任务，或者在一段时间内的极端状态值。本系统的主要任务是从互联网下载数据并对其进行抽取，需要本系统在的时间内完成一个任务任务流程，同时要保证结果数据达到的精确度。
（2）安全性需求。该需求是指系统品消除潜在风险的能力和对风险的承受能力，一般包含保密性、可靠性和完整性三个子特性。保密性是指，本系统在提供供其他系统调用接口获取数据的功能时，需要保证不能被授权用户以外的任何人访问，以造成关键数据的泄漏。可靠性是指，授权用户可以不受阻止的访问数据、与其它软件的兼容的能力和产品的强壮度。完整性指的是安预期目标完成任务的能力。
（3）系统的完整性需求。该需求是指指为完成业务需求和系统正常运行本身要求而必须具有的功能，这些功能往往是用户不能提出的。在本系统中，应该包括用户管理、数据备份、恢复、日志管理等基本功能。其中，用户管理功能使得系统可以有效控制用户的使用，使系统处于一个安全、负载合理的运行状况，还能提高系统的应用适应性。
3.4 本章小结
本章针对进行了需求分析。首先，分析了产学研平台的业务，然后根据对系统的功能性需求进行分析，描述了系统的用户角色和主要功能，主要功能包括数据获取功能、数据预处理功能、主页抽取功能、术语抽取功能、数据接口与应用管理接口，最后阐述了系统的非功能性需求。

第四章 信息抽取系统设计
4.1 系统总体设计
4.1.1 三螺旋数据服务平台简介
本文所研究的信息抽取系统是“三螺旋数据服务平台”的子系统。“三螺旋数据服务平台”通过获取、整合开放互联网中的数据，解决产学研活动提供基于搜索与语义挖掘服务中的问题，具体包括：1）如何从现有的非结构化网络中自动提取研究人员简介；2）如何整合来自不同来源的信息；3）如何基于构建的学者社区提供有用的搜索服务；4）如何通过平台挖掘学者与企业社区以便为用户提供更强大的服务。
三螺旋数据服务平台的全景图如图 4-1所示，主要包括数据组织系统、数据处理系统、数据分析系统、应用服务系统和Web应用，系统的设计采用C4软件架构模型。

图 4-1三螺旋数据服务平台全景图
其中，数据组织系统与数据处理系统是平台搭建的基础。数据组织系统负责的是所需要的数据收集和整理，数据的范围包括通过网页爬虫获取的学者数据与学术数据和运营活动产生的活动数据，对收集到的数据进行整合转变为有序有规则的组织结构，使平台其他部分能更方便的使用数据。数据处理系统是指对收集到的数据进行预处理、重名消歧、信息抽取等工作，将数据处理为结构化的可供挖掘分析的数据，处理之后的数据满足可用性、一致性、完整性的要求。
数据分析系统是平台能够提供产学研服务的核心部分，主要负责平台的数据计算分析过程，具体包括社会网络分析、学者画像挖掘、学者研究兴趣挖掘等部分。Web应用服务主要包含社区检索、学者现实社区、学者画像、运营活动管理，是平台直接为最终用户提供服务的重要部分。用户在Web前端页面可以通过社区检索功能查看学者现实社区，再进一步查看学者画像，或者对运营活动进行管理。
数据存储部分负责数据信息的持久化存储和优化。在数据存储方面，对系统中不同特点的数据采用不同的存储方式，包括图数据库Neo4j、关系型数据库MySQL以及文件系统。日志系统负责记录系统中软件和系统问题的信息，同时还可以监视系统中发生的事件。运维系统承担系统维护与版本更新的任务，是平台正常运行的保障。
4.1.2 信息抽取系统总体框架
本系统将来源于高校的官方教师主页及其文献信息的数据，通过数据采集获得相关数据，然后对这些数据信息进行预处理、信息抽取与存储，作为支撑数据用于《三螺旋大数据服务平台》，为用户提供服务。
 图4-2信息抽取系统容器图

本系统整体框架图如图4-2所示。图中虚线框将本系统与外部系统以及用户划分开，虚线框内部的各个方块表示系统内部的各个容器，虚线表示容器的调用关系和数据交互关系。各个容器能够进行简单的数据交互，容器与容器之间低耦合，容器内部的各个功能模块之间高度内聚。
在本系统中，数据采集容器负责获取开放互联网中的主页数据。数据采集容器以学者库为范围，搜索并识别与学者对应的主页链接，通过链接下载完整主页。数据预处理容器需要对采集的主页数据进行正则化和去重去躁等处理，返回规范化的已知编码类型的网页正文。主页抽取容器是指利用文本分类、实体识别等方法从预处理后的主页数据中抽取出学者的描述信息，包括，姓名、学校、年龄、教育经历等，并存储在数据库中。术语抽取容器的操作对象是经过重名消歧处理的论文和专利文本，容器会利用术语词库和一些统计学方法，对文本进行术语抽取，以得到每篇文献的领域术语列表，最后以学者为单位将术语组织成学者术语集合，存储在数据库中。数据存储容器包含数据库存储和文件系统存储，存储的是学者画像标签元数据、抽取结果数据和系统中部分容器需要用到的模型文件与规则文件。API接口容器主要为用户提供基于HTTP的数据访问接口服务与基于抽取任务的任务控制服务，满足平台的两类用户的需求。
4.1.3 信息抽取系统部署框架
系统的部署架构如图 4-3所示，图中展示了系统容器到执行环境的映射。

图 4-3信息抽取系统部署架构图
4.2 系统详细设计
4.2.1 系统功能概述
系统总体功能结构图如图 4-4所示，分为数据获取与预处理、主页抽取、术语抽取以及应用服务四个模块。

图 4-4 系统功能结构图
4.2.2 数据获取与预处理模块设计
学者的主页通常分布在高等院校、科研机构和部分信息平台中，使得主页数据具有数据源多、数据格式不一致的特点，且具有时效性。数据获取与预处理模块首先通过主页检索识别组件检索并识别主页链接，然后通过主页采集组件请求主页链接下载主页，最后调用预处理组件对下载的主页进行规格化处理，并临时存储。模块具体的工作流程如图 4-5所示。

图 4-5 数据获取与预处理模块设计
1）主页检索识别
数据源分析。为了能够获得多源的主页数据，本系统借助第三方搜索引擎，设置请求关键词等参数，通过学者姓名以及机构信息得到学者相关的结果列表，再识别出结果中的学者主页链接，获得主页数据。请求参数如表 4-1所示。
参数	解释
完整关键词	搜索结果包含完整的关键词
时间	限定要搜索的网页的时间，包括最近一天，最近一周，最近一月，最近一年，全部时间
文档格式	搜索网页格式，包括HTML、DOC等
表 4-1百度搜索引擎请求参数
主页识别。在第三方搜索引擎的结果列表中，通常会给出对应标题和页面主要内容摘要，如图 4-6。其中，学者主页的结果相较于其他的搜索结果在URL结构、title和摘要文本内容上有明显的区别，具体如表 4-2所示。为了能够减少请求次数，更高效地获取学者主页数据，充分利用搜索引擎结果列表的特点，采用了先识别主页链接，再请求相应链接获取主页的方式。

图 4-6搜索结果展示

表 4-2搜索结果特征
属性	特征
URL结构	URL中包含hompage、人名等关键词，
域名往往表现为公司机构、高校等子站点，如edu、org、baike.com等
标题	标题中包含人名、机构名、“个人主页”、“个人简历”等关键词
内容摘要	标题中包含人名、机构名、职称等关键词
在文本分类任务中，特征的选择主要考虑某个词在文本或文本集中的词频。TF-IDF是一种最为常见的基于词频和文档频率的特征提取方法。
同时，为了解决TF-IDF在文本语义方面的表征不足，采用了词向量特征与TF-IDF特征融合的方法，具体细节见4.2.3小节。
在搜索结果提取特征之后，进入文本分类。主页链接识别中的分类任务是二分类，采用XGBoost算法。从实际应用来看，XGBoost在各种实际问题中都有良好的表现，不仅文本分类周期短，而且在训练集表较少的任务中也能取得较好的效果。要将XGBoost模型应用在主页识别任务中需要经过训练和预测两个阶段。训练阶段根据准备好的语料，通过训练得到分类模型，再将模型应用在本系统中。系统中主页链接识别的流程如图 4-7所示。

图 4-7主页链接识别过程示意图
2）主页采集
本系统采用Python语言实现，目前有许多成熟的可通过Python进行调用的成熟的爬虫框架如 BeautifulSoup、PyQuery、PySpider、Cola、Scrapy等。其中，BeautifulSoup和PyQuery需要结合Requests和Re工具完成网页的请求与解析，适合小型网络爬虫的设计；PySpider规则易懂，使用简单；Scrapy是一个通用的框架自定义程度高，其底层采用twisted异步框架，有着并发优势，且具有扩展性强、跨平台的优势，支持日志、异常处理等功能，是目前 Python语言中最主流的爬虫框架之一，因此，本文选用 Scrapy 爬虫框架来实现主页采集。图 4-8是主页采集组件的工作示意图。

图 4-8主页采集组件工作示意图
3）网页预处理
学者主页数据的来源不同，网页编码、简繁字体、文件格式也不尽相同，在进行下一步工作之前，需要对其编码、字体、文件格式进行统一。同时，也需要对电话号码、手机号码等特殊字符串进行规整处理。Python中有许多成熟的工具用于处理不同的文本问题。如表 4-3所示。




表 4-3 python中用于处理文本的工具
名称	详细介绍
Chardet	用于检测字符编码
phonenumbers	用于解析、格式化、存储和验证国际电话号码
Re	通过编写正则表达式，检查、匹配或提取对应模式字符串
zhconv	提供了基于 MediaWiki 词汇表的最大正向匹配简繁转换，支持多地区中文简繁转换
HTML Tidy	HTML Tidy是一个能够修正不规范的HTML文件,以及调整缩进的工具
4.2.3 主页抽取模块设计
主页抽取模块的目的是给三螺旋数据服务平台建立学者画像与检索系统提供学者个人信息。学者的信息大多包含在个人主页中，例如姓名、出生年月、职称、联系方式以及教育经历等信息，典型的学者主页如图 4-9所示。

图 4-9 学者主页示例
由于个人主页来源多样，学者主页中信息的表现方式也不一样，学者画像抽取的准确性和实用性成为学者画像技术一个重大挑战。通过观察发现，多数学者主页中，基本信息、联系信息会单独成行；而教育经历、工作经历多以列表形式呈现；同时，某些属性之间有依赖关系，比如在描述个人的教育经历时，博士学位、获得博士学位的专业和获得博士学位的日期很可能出现在同一句话中；邮箱、电话等信息可以用固定的正则表达式表示；某些主页中，学者姓名会在页面头部和出版物信息中反复出现多次。
基于上述特性，一个可行的抽取主页的方法是，先对主页文本进行分块处理，得到多个文本单元。再对文本单元分类，得到一系列有确定标签的文本单元，再对不同的文本单元进行不同的操作，得到规整有效的信息。学者主页抽取工作过程如图 4-10所示。

图 4-10 主页抽取策略流程图
1）文本分块
网页通过多样灵活的样式，呈现出比普通文本更丰富的内容，通过观察发现，网页也可以认为是多个不同的文本块组成。在学者主页上，这种特征尤其明显，如图 4-9所示，不同的描述信息有明显的区分。通过观察，这种文本块分隔的特征与网页标签及其样式有直接的关系，可以方便的通过网页标签来对文字进行分割。
一般来说，与网页文本有关的标签分为块级标签和内联标签，常见的块级标签和内联标签如表 4-4所示。网页可以看做是块级元素的集合。块级元素又称行元素，一般一个标签占据一行的位置；内联元素又称行内标签，一行可以有多个内联标签。块级元素和内联元素之间可以相互嵌套，也可以通过指定属性，使多个块级元素在同一行或者内联标签独占一行。同时，标签之间存在着父子或者兄弟的关系。这些特点成为网页分割的基本依据。
表 4-4网页标签类型
块级标签	h1~h6、p、div、list、table
内联标签	span、a、img、b、br
网页分割的粒度选择通常有两种思路：一种是按照块级标签，划分出最小信息文本块；一种是根据标签所包含的字标签文本的相关性确定最小粒度。在基于标签分割的基础上，对已划分的文本块根据特殊标志符以及自然语言分割符等划分，例如“\n”，“。”等进一步划分。
DOM(Document Object Model)是一个与语言和平台都无关的协议，如图所示，它将HTML中的所有元素都定义成节点，将HTML文档表达为一个树形结构，树节点的父子关系和兄弟关系来表达HTML标签的嵌套关系和并列关系。基于DOM树可以实现分割算法，完成文本块的提取与分割。

图 4-11 HTML文档及DOM树转换示意图

算法4.1 HTML文档分句
输入：Html文档
输出：文本单元集合（TextSet）
算法流程：
1、对Html文档，使用正则表达式去除特定标签、去除注释、替换实体、替换指定标签；
2、生成DOM树，从根节点开始层次遍历节点；
3、对节点，判断是否没有孩子节点，若没有，则到4；若有，则到6；
4、判断是否为块级属性，若是，节点文本加入TextSet；若不是，则到5；
5、判断是否有父节点，若没有，节点文本加入TextSet；若有，则与父节点合并；
6、层次遍历孩子节点，到3；
7、遍历结束后，对TextSet中的元素据特殊标志符以及自然语言分割符进一步分割，得到最终的TextSet。
2）分词与实体识别
广义上的实体是指姓名、地址、机构名和专有名词，也可以包括时间、日期、邮箱、电话等，在本文的主页抽取任务中，最终的抽取结果是由多个实体名词组成的，因此实体识别是关键的环境，实体识别的准确率影响着最终的结果。
实体识别的方法大致可以分为基于规则、基于统计的序列标注以及深度学习三类。其中基于规则的实体抽取依赖于大量的手工编写规则，序列标注与深度学习用于实体识别任务需要相当数据量的训练集。国内外的研究所或大学已经在实体识别领域取得一定的成功，并开源了自己的研究成果，例如哈尔滨工程大学的LTP、斯坦福大学的CoreNLP。本文将使用开源的实体识别工具完成实体识别任务。
由于书写的不规范性和文字叙述的复杂性，在实际操作中发现仍有部分实体难以被NER工具识别出来。本文将使用正则表达式，对日期、邮箱、电话等实体进行初步识别。同时，引入自定义的机构名称库，提高机构实体的识别准确率。分词与实体识别的流程示意图如图 4-12所示。

图 4-12分词与实体识别流程示意图
由于系统分词和实体识别的是由多种方法组成的复合策略，不同的方法得到的结果可能存在冲突，需要制定一系列规则解决冲突，得到最终的结果。具体的规则如下所示：
1、首先根据自定义词库标注出词语，自定义词库中的词语存在部分词为另一些词语前缀的可能性，在匹配时满足最大匹配规则。
2、根据正则将日期、邮箱、固定电话和手机号字符串识别为实体。
3、实体标注及分词工具得到标注序列，与自定义词库和正则识别出的实体重合的词语作停用词处理。
3）文本单元分类
文本单元类别主要有无关文本、单信息文本以及多信息文本，文本单元分类的意义在于确定文本单元的具体类别，以便于后续操作中的去处多余的文本信息，对应到学者画像相应的标签中。具体的文本单元类别如表 4-5示。
表 4-5文本单元分类标签
标签名	说明	标签名	说明
NAME	姓名	POSITION	职称和职位
SEX	性别	EDU_EXP	教育经历
BIRTHYEAR	出生年月	WORK_EXP	工作经历
PHONE	电话号码	PUBULISH	出版信息
EMAIL	邮箱	MUTIPLE_INFO	多信息文本
DEGREE	学历	IGNORE	无关信息文本
在使用预训练语言模型的基础上，用主页语料重新训练适用于本文任务的词向量。考虑到对训练集数据量的要求，本文采用Skip-Gram的建模方式，训练主页语料下的词向量模型。词向量的训练语料集为学者主页的句子集合，在训练前对句文本做分词和去除停用词处理。在词向量的训练过程中，由于电话号码、邮箱以及日期这样的字符串有唯一性，同一个字符串在语料中的词频较低，为了能更好的表征语义，需要将其用标志字符串替换，如将“电子邮箱：example@163.com”替换成“电子邮箱：[EMAIL]”，“1999年毕业于东南大学”替换为“[YY]毕业于东南大学”。词向量的训练过程如图 4-13所示。

图 4-13词向量训练流程图
目前，预训练语言模型已经成为自然语言处理中的主流，借助预训练语言模型，可以更好的把握词义信息。BERT是目前较为热门的预训练词向量模型，与Skip-Gram的思想相比，BERT模型考虑到词语的次序关系，对词的向量表征有很好的效果，但在训练过程中。同时，BERT由Google投入了大规模的语料和机器完成训练，在词义的表征上更接近真实世界，也适用于本文的后续任务中，所以本文将对BERT预训练向量和自定义语料训练向量进行拼接，得到新的词向量。
如第二章节所述，TF-IDF是一种的基于词频和文档频率的特征提取方法，能够很好的表征出同一个词在不同文档中的重要程度。除此之外，文本单元中词语的实体特征和文本单元在主页的位置也是此次文本分类任务中的重要特征。其中，实体类型有人名、地名、机构名、邮箱、时间等。文本单元的实体特征是统计文本的实体个数，得到实体分布向量，维度为8维，统计的实体类型如表 4-6所示。
表 4-6文本特征实体表
实体类型	人名	机构名	时间	年份	年月	年月日	邮箱	电话
实体标签	NR	NT	TIME	YY	YYMM	YMD	EMAIL	PHONE
特征经过拼接处理，得到文本单元的向量作为分类模型的输入，完成文本分类任务，具体拼接过程如图 4-14特征拼接所示。由于特征维度的限制，还需要对原始高维度的向量空间进行压缩，本文选用PCA降维的方式，动态调整目标维度并观察数据失真度的变化曲线，确定最终的维度。对于实体特征和位置特征需要经过归一化处理。自训练词向量为静态词向量，每个词都有固定的值，在表示文本单元的语义特征时需要将文本中词的词向量相加。

图 4-14特征拼接
4）去除冗余信息
在前面得到了有确定标签的文本单元基础上，要得到描述学者画像所需要的目标属性，还需要对文本单元做进一步的处理。去除冗余文本得到最终目标属性的整体流程如图 4-15所示。
对于部分文本单元，可以将其实体名词与专用名词，目标属性一一对应，具体对应关系如表 4-7所示。多信息文本单元的存在，往往是因为在主页中，一句话描述了多个有效信息，对于这部分文本单元，在直接将实体与最终属性对应的时候容易产生歧义，例如描述学历信息的上下文有描述出生年份的信息。在处理多信息文本单元时，本系统会进一步将句子切分成更小单元，再对这部分文本单元进行分类，得到单信息文本。正确性检查是指对根据主页抽取策略提取的实体和专有名词作入库检查，具体包括：（1）检查姓名和机构名是否符合规则；（2）检查姓名与机构名是否符合系统输入参数。

图 4-15去除冗余信息流程图

表 4-7文本单元标签和学者画像标签对应关系
文本单元类型	实体与专有名词组合	学者画像标签
NAME
PUBULISH	[NR]	Name
GENDER	男/女	Gender
BIRTHYEAR	[YY]/[YYMM]/[YMD]	Age
PHONE	[PHONE]	Phone
EMAIL	[EMAIL]	Email
DEGREE	学士/硕士/博士	Degree
POSITION	教授/副教授/讲师...	Position
EDU_EXP	([YY], [YY], [NT], 学士/硕士/博士, 专业)
([YYMM], [YYMM], [NT], 学士/硕士/博士, 专业)	(StartYear, EndYear, Ins, Degree, Major)
WORK_EXP	([YYMM], [YYMM], [NT], 教授/副教授/讲师...)	(StartYear, EndYear, Ins, Position)
4.2.4 术语抽取模块设计
本文的术语抽取是指对平台数据库的学术论文中的术语进行刻画，使术语区别于其他词语，术语抽取效果的好坏影响着学者研究兴趣挖掘的准确性。当前的术语抽取技术主要有三类：基于词典的术语抽取、基于统计的术语抽取、基于机器学习的术语标注和深度学习算法对领域实体的标注。考虑到基于统计与深度学习的算法要求一定数据量的训练集带来的标注难度，本文中的术语抽取采用语言规则和统计策略相融合的中文领域术语抽取方法。主要流程图 4-16所示。

图 4-16术语抽取流程图
（1）数据源
论文数据 本模块处理的数据来源于三螺旋数据服务平台的学术论文，本系统调用平台的重名消歧服务子系统，获取与作者对应的论文数据。
自定义词库 本文的术语抽取设计基于对论文的分词和词性分析，为了能将论文文档中的专业术语更准确的切分，引入了自定义的学科领域词典，词典来源于术语知识服务平台（http://www.termonline.cn/）。该平台的术语由平台向各个专业人员征集，并对术语进行了严格的学科划分，部分术语会同时术语多个学科。
（2）词性规则
术语作为某个领域反复使用、形式较为固定又表达某特定概念的词语， 其组成结构一般具有固定的词性组合特点，例如“名词+名词”，但这种规则较为宽松，由于每个专业用语的习惯不同，不能很好的组成新的词。在大量论文文本的基础上，发现标题大多含有体现该论文领域特色的长术语，且其嵌套词组也多为术语。基于这一现象，设计了针对不同领域文本自动生成词性规则的算法，具体计算过程如算法4.2所示。



算法4.2 词性规则自动生成算法
输入：标题集合 ，论文集合
输出：HashMap，其中key=词性规则，value=词性规则统计频次
算法流程：
FOR 每篇文章  DO
对文章进行词性标注
得到文章的词性序列 和词语序列
FOR   DO
    P=0
IF   OR
         =标题 与子串 的最长公共子串
        IF 词性规则 不在HashMap中
            HashMap.add( )
        ELSE
           HashMap[ ] = HashMap[ ] +1
END

（3）MI和C-Value*指标
MI 即互信息，是指两个事件集合之间的相关性，本系统采用互信息度量词语的单元性。对于根据规则提取的长候选术语计算其互信息向量，若互信息向量中最小的分量低于设定阈值，则过滤该候选术语。
C_Value*  C_Value的计算考虑到了简单术语与复杂术语之间的关系，取得了。若 为不被任何其他串包含的字符串，则称 为“极大串”，字符串 包含字符串 ，则 为 的“母串”， 为 的“子串”。C-Value通过减小嵌套串词频的影响，来平衡简单术语和复杂术语的关系，但其并未考虑到词语的专业领域相关性。所以，本文通过加入逆文档率改进C-Value。改进的C-Value计算如式(4.1)和式(4.2)所示，其中 表示背景语料的文档数， 为a在背景语料中的词频，M为专业语料的文档总数，以平衡不同专业文档数不一致对结果的影响。
 ， 为极大串
(4.1)
         ， 为嵌套串
(4.2)

（3）术语数据结构设计
传统的C-Value算法在计算时只需要对一个学科或领域的专业文本做词频统计，而改进的C-Value*算法由于引入了背景语料，在统计时应先将已有论文按学科统计词频和词在该学科的文档频率。为了便于增量更新平台论文数据时，术语抽取的数据能够同步更新，为候选术语建立以词为键的数据结构，保存相关的数据。词语a与某一学科下的词频tf(a)、文档频率df(a)、包含a的母串s词频f(s)的对应关系如图 4-17所示。在实际实现中，使用MongoDB文档数据库保存，相关表的设计如图 4-22所示。

图 4-17术语与相关统计量的关系
4.2.5 应用服务模块设计
应用服务模块通过实现HTTP接口，调用系统其它子模块，为用户管理学者数据、管理抽取服务以及为平台其他子模块提供数据访问的功能，应用服务模块的整体架构如图 4-18所示。

图 4-18应用服务模块架构图
在REST风格的HTTP接口设计中，将互联网中的实体定义为资源，一个实体统一用一个名称表示，且具有唯一性。本系统所涉及的数据与服务，有学者数据scholar、术语数据term、学者主页抽取服务scholar_extractor以及术语抽取服务term_extractor。图 4-19描述了系统中应用服务模块包含的主要资源，由根节点/api到其他节点的路径组成遗传URL地址，即该资源对应的 URL 地址。

图 4-19模块资源结构图
对于学者数据资源，系统提供获取学者数据的接口，允许用户按学校或者学院查询学者数据，具体的参数设计如表 4-8所示。
表 4-8学者数据资源接口参数表
子目录	方法	参数	类型	说明
/scholar/list	GET	school_name	string	非必须，学校名称
		ins_name	string	非必须，学院名称
		offset	int	非必须，偏移量
		limit	int	非必须，一次查询数据量
对于术语数据资源，系统提供获取术语数据的接口，允许用户学科查询学者术语数据，同时可以选择返回词库或者有c_value权重的术语数据，具体的参数设计表 4-9所示。
表 4-9术语数据资源接口参数表
子目录	方法	参数	类型	说明
/term/list	GET	discipline_code	string	必须，学科代码
		offset	int	非必须，偏移量
		limit	int	非必须，一次查询数据量
		c_value	int	非必须，默认为0，仅返回词库
对后台管理资源，主要有三个部分：学者数据管理、术语数据管理和抽取服务管理。
学者数据管理 是指后台对学者数据的管理操作，具体操作如表 4-10所示。管理学者资源的接口参数具体如表 4-11所示。


表 4-10学者数据管理操作
查询	1、根据scholar_id查询学者
2、根据scholar_name查询学者列表
3、根据学者数据状态status查询数据列表
4、查询冲突数据
更新	1、根据scholar_id更新学者基础数据
2、根据contact_id更新联系人数据
3、根据edu_exp_id更新学历信息
4、根据work_exp_id更新工作信息
删除	1、根据scholar_id删除学者基础数据
2、根据contact_id删除联系人数据
3、根据edu_exp_id删除学历信息
4、根据work_exp_id删除工作信息
插入	1、新增至少包含姓名、学校、学院的学者信息

表 4-11学者资源管理接口参数表
子目录	方法	参数	类型	说明
admin/scholar	GET	scholar_id	string	非必须，学者id
		scholar_name	string	非必须，学者姓名，参数中学者姓名与学者id至少有一个
admin/scholar	PATCH	scholar_id	string	必须，学者id
		scholar_name	string	必须，学者姓名
		school_id	int	非必须，学校id
		ins_id	int	非必须，学院id
		title	string	非必须，职称
		degree	string	非必须，学位
		birth_year	string	非必须，出生年份
		email	array	非必须，邮箱
		phone	array	非必须，电话号码
		edu_exp	array	非必须，教育经历
admin/scholar	POST	scholar_name	string	必须，学者姓名
		school_id	int	非必须，学校id
		ins_id	int	非必须，学院id
		title	string	非必须，职称
		degree	string	非必须，学位
		birth_year	string	非必须，出生年份
		email	array	非必须，邮箱
		phone	array	非必须，电话号码
		edu_exp	array	非必须，教育经历
admin/scholar	DELETE	scholar_id	string	必须，学者id
admin/scholar/list
	GET	school_id	int	非必须，学校id
		school_name	string	非必须，学校名称
		ins_id	int	非必须，学院id
		ins_name	string	非必须，学院名称
		discipline_code	string	非必须，学科代码
		limit	int	非必须，一次查询数据量，默认为10
		offset	int	非必须，偏移量，默认为0
术语数据管理 是指后台对术语出具的管理操作，具体操作如表 4-12所示，参数设计如表 4-13所示。

表 4-12术语数据管理操作
查询	1、根据学科代码查询术语基础词库列表
2、根据学科代码查询术语与C-Value值的列表
更新	1、根据词语更新术语基础词库
删除	1、根据词语删除术语基础词库
插入	1、新增词语与学科信息到术语基础词库

表 4-13术语数据管理接口参数表
子目录	方法	参数	类型	说明
admin/term/words	GET	discipline_code	string	必须，学科代码
		offset	int	非必须，偏移量
		limit	int	非必须，一次查询数据量
		c_value	int	非必须，默认为0，仅返回词库
admin/term	UPDATE	term_discipline_pairs	array	必须，更新
admin/term	DELETE	term_discipline_pairs	array	必须，更新
主页抽取服务管理 支持后台通过设置相关参数启动主页抽取任务、查询主页抽取任务状态以及对历史数据进行更新。接口相关参数定义如表 4-14所示，若name为空，而school_name不为空，则更新相应学校的学者数据。
表 4-14主页抽取服务管理接口参数表
子目录	方法	参数	类型	说明
admin/scholar_extractor	GET	time_start	int	非必须，获取的主页的创建的起始时间。默认获取所有时间的主页。
		name	string	非必须，学者姓名
		school_name	string	非必须，学校名称
		ins_name	string	非必须，学院名称
术语抽取服务管理 支持后台通过设置相关参数启动主页抽取服务，对于术语抽取服务，支持更新词库，具体接口参数设计如表 4-15所示。
表 4-15术语抽取服务管理接口参数表
子目录	方法	参数	类型	说明
admin/extractor/term	GET	discipline_code	string	非必须，学科代码
		start_year	string	非必须，论文发表起始时间。默认获取近五年的所有论文。

响应消息 服务返回的消息包含状态码Status、消息描述Message和数据data，其中状态码和消息描述的定义如表 4-16所示。
表 4-16状态码和消息描述定义表
Status	Message
1001	用户名或者密码错误
1002	服务器内部错误
1003	无效参数
1004	请求参数缺失
1005	数据库连接失败
1006	数据库操作失败
1007	数据库操作异常
200	请求成功
2001	用户新建或修改数据成功
2002	用户删除数据成功
400	HTTP 400 Bad Request
401	没有权限
403	HTTP 403 Forbidden
404	HTTP 404 Not Found
4.3 数据存储设计
本系统所使用的数据分为两种，一种是关系型数据如学者基本信息数据，另一种是文件型数据如爬取的主页数据、术语抽取结果数据。
学者信息表 学者信息包括学者基本信息、联系方式、教育经历、工作经历多项信息，其中联系方式和教育经历与学者之间存在一对多的关系，采用关系数据库存储。相关属性的定义见表 4-17、表 4-18、表 4-19、表 4-20。
表 4-17学者基本信息表定义
字段名	数据类型	字段含义	允许空
id	int	学者ID	否
name	varchar(32)	学者姓名	否
school_id	int	学校ID，外键，对应学校信息表	否
ins_id	int	机构ID，外键，对应机构信息表	否
title	varchar(32)	职称	是
birth_year	varchar(16)	出生年份	是
gender	tinyint	性别，0为女性，1为男性	是
pic_path	varchar(128)	个人照片存储路径	是
url	text	主页链接	是
status	tinyint	记录状态（0新增记录，1已验证记录，2已删除记录，）	否

表 4-18联系方式表定义
字段名	数据类型	字段含义		允许空
id	int	联系方式ID	否
s_id	int	学者ID，外键，与学者基本信息表对应	否
type	tinyint	联系方式类型，0为“邮箱”、1为“电话”	否
value	varchar(64)	联系方式字符串	否
status	tinyint	记录状态（0新增记录，1已验证记录，2已删除记录，）	否

表 4-19教育经历表定义
字段名	数据类型	字段含义		允许空
id	int	教育经历ID	否
s_id	int	学者ID，外键，与学者基本信息表对应	否
degree	varchar(16)	获得学位	否
start_yymm	varchar(16)	教育经历开始时间	是
end_yymm	varchar(16)	教育经历结束时间	是
school_name	varchar(64)	就读学校	否
school_id	int	就读学校ID	是
major	varchar(64)	专业	是
status	tinyint	记录状态（0新增记录，1已验证记录，2已删除记录，）	否

表 4-20工作经历表定义
字段名	数据类型	字段含义	允许空
id	int	工作经历ID	否
s_id	int	学者ID，外键，与学者基本信息表对应	否
start_yymm	varchar(16)	工作经历开始时间	是
end_yymm	varchar(16)	工作经历结束时间	是
school_name	varchar(64)	工作机构	否
school_id	int	就读学校ID	是
position	varchar(64)	职位	是
status	tinyint	记录状态（0新增记录，1已验证记录，2已删除记录）	否

学者主页数据 学者主页数据是指从互联网中爬取的数据，存储在关系数据库，具体定义见表 4-21，包括学者姓名、主页链接、主页title、html文档、结果排序。
表 4-21学者主页数据表定义
字段名	数据类型	字段含义	允许空
id	int	主页ID	否
scholar_name	varchar(32)	学者姓名	否
school_name	varchar(64)	学校名称	否
ins_name	varchar(64)	学院名称	是
url	text	主页链接	是
title	text	主页title	是
html_doc	longtext	html文档	是
index	index	搜索结果数据序号	是
status	tinyint	记录状态（0新增记录，1已处理记录）	否

抽取任务数据 抽取任务数据表中存储着抽取任务的参数和任务状态，用于应用服务模块管理抽取器，具体定义见表 4-22。
表 4-22抽取任务数据表定义
字段名	数据类型	字段含义	允许空
id	int	任务ID	否
type	tinyint	任务类型，0为主页抽取，1为术语抽取	否
params	text	任务参数	否
create_time	int	任务创建时间，timestamp类型	否
execute_time	int	任务执行时间，timestamp类型	是
status	tinyint	记录状态，0新建任务，1进行中，2正常结束，3异常结束	否

学术术语数据 根据学术术语的使用场景，学术术语抽取的结果数据和术语词库可以采用文件的方式存储，对于仅包含词语的词库和带有术语权重的文件分别存储，便于查找更新，具体定义如图 4-20、图 4-21。

图 4-20术语C-Value数据表定义


图 4-21术语词库数据表定义
候选术语数据 为了便于增量更新论文时学术术语的计算，设计了保存候选术语相关统计量的数据存储结构如图 4-17所示，这种数据结构有着键值对应关系且具有扩展性，所以采用适用这种数据类型的文档数据库MongoDB存储，具体设计如图 4-22所示。

图 4-22候选术语统计数据存储表
4.4 本章小结
本章主要针对面向学者画像的信息系统需求进行了系统设计。首先，对系统进行了总体设计，包括对三螺旋数据服务平台的介绍以及系统的总体框架和部署框架；然后，对系统进行了详细设计，介绍了系统的功能和各功能模块的详细设计；最后，对系统涉及的数据表进行了设计。
 
第五章 信息抽取系统实现
5.1 系统开发环境
1、硬件环境
CPU型号：Intel(R) Core(TM) i5-7300 CPU
内存容量：8.0GB
系统类型：64位操作系统，基于x64的处理器
2、软件环境
操作系统：Microsoft Windows 10
数据库：MySQL、MongoDB
开发工具：PyCharm、Sublime Text 3、Chrome、Postman
编程语言：Python 3.6
5.2 数据获取与预处理模块实现
本模块主要基于Scrapy框架主页获取，通过拼接搜索链接请求百度搜索引擎，实现主页数据获取，再获取过程中调用主页链接识别。
5.2.1 采集器
数据定义 Scrapy框架中定义Item类是数据容器，根据请求获取的数据格式为其统一建模。本文中下载的主页数据类PageItem定义如图 5-1所示。

图 5-1主页数据类定义
爬虫请求 PageSpider是继承自scrapy.Spider的自定义爬虫请求类，表 5-1展示了该类主要属性和方法。
表 5-1爬虫请求类属性和方法定义
名称	说明
name	唯一标识爬虫字符串
start_url	爬虫启动时的url
__init__()	初始化爬虫类的方法
parse()	解析搜索结果页的方法
parse_page()	解析主页的方法
在__init__()方法中，对传入的参数进行拼接处理，生成start_url，参数有教师姓名列表names、机构列表schools、起始时间start_time。其中，url拼接规则为:url="http://www.baidu.com/s?ie=utf-8&f=3&tn=baidu&wd={姓名} {学校名}{学院名}&gpc=stf={Unix.stamp(起始时间)},{Unix.Stamp({现在时间})}"。
爬虫中start_url集合中的每个URL再完成下载后会生成的 Response对象，作为唯一的参数传递给parse()方法。在该方法可以通过selector选择器提取搜索结果页中的列表，调用主页链接识别方法决定是否过滤主页，得到需要进一步处理的url的Request对象。
parse_page()在parse()中请求主页链接后，返回响应结果会作为该函数的唯一参数传入，在这基础上对结果PageItem进行封装。
数据存储 爬虫组件爬取的数据多为半结构化或非结构化数据，系统选择使用MongoDB数据库存储爬取到的数据，具有灵活的数存储方式。实现时，在pipelines.py文件中创建了一个MongoDBPipeline()类，构造初始函数初始化类，连写MongoDB数据库，存储解析后的数据。
反爬虫 为了避免某些网站的反爬虫策略，制定了一系列的规则来应对不同的反爬问题，主要措施有：
（1）设置下载延迟参数。即下载器在下载同一个网站不同页面时需要等待时间。对于延迟时间，如果下载等待时间长，则不能满足短时间大规模抓取的要求；而太短则大大增加了被禁止访问的几率，根据经验，在此次爬虫任务重，下载延迟参数可以设置为2s。具体实现过程时只需在Scrapy结构中的settings.py文件设置DOWNLOAD_DELAY 为2。
（2）禁止使用cookies。对部分使用cookies 识别爬虫轨迹网站，可以有效的防止被网站禁止访问。具体实现时可以在settings.py配置文件中设置COOKIES_ENABLES为False；
（3）User-Agent池。Http请求头使用不同User-Agent池，可以在重复访问同一个网站时，有效防止被服务器识别为爬虫程序。在实现过程中，将User-Agent池写入配置文件setting.py中；同时启用爬虫中间件，在爬虫请求前，对同一网站的请求替换不同的User-Agent。
5.2.2 主页链接识别
主页链接识实现了PageLinkClf类，对解析函数parse()中解析的链接、标题、摘要做分词和去除停用词预处理，然后提取TF-IDF特征和自定义特征，加载训练好的XGBoost模型，将输入的数据分类成主页或者非主页，类的具体定义见表 5-2。模型的训练和加载使用开源的xgboost包。
表 5-2主页链接识别类定义
名称	说明
model	bytes，分类模型
item	dict，主页数据
is_page()	判断是否为主页
load_model()	加载模型
get_feature()	获取特征向量
5.2.3 预处理
主页数据预处理主要实现PagePreprocess类,，实现主页编码识别与转换、简繁转换、电话号码以及邮箱格式的规范化。用到的工具有chardet、re、zhconv、phonenumbers、htmltidy，预处理具体流程见表 5-3。
表 5-3主页预处理类定义
名称	说明
html_doc	string，HTML文档
encode2utf8()	识别编码，并将编码转换为utf-8
normalize()	规范化特殊字符串，替换掉原文本中不规范的字符串
html_tidy()	规范化HTML文档的格式和缩进
5.3 主页抽取模块实现
5.3.1 文本分句
文本分块实现了PageCutter类，实现输入一篇主页的HTML文档，得到相应的文本单元列表，同时实现PageItem类，用于保存分块的结果。PageCutter类定义如表 5-4所示，PageItem类的具体定义如表 5-5所示。
表 5-4 PageCutter类定义
名称	说明
pageItem	PageItem类实例
soup	BeautifulSoup类实例
features	string，创建soup的参数，指定了生成DOM时使用的解析器
get_page_item()	返回切分后的数据
rec_traverse()	遍历标签
fine_cut()	进一步切分
page_clean()	去除HTML文档冗余信息

表 5-5 PageItem类定义
名称	说明
title	string，页面标题
texts	list，文本单元集合
doc	string，HTML文档

在get_page_item()中，输入的HTML文档生成BeautifulSoup类实例，将文档转换成DOM树，其中features属性指定了生成DOM时使用的解析器，包括"lxml"、"lxml-xml"、"html.parser"、"html5lib"、"html"、"html5"、"xml"，默认为"html.parser"。
page_clean()实现了对HTML中的注释和无用标签的清洗，无用标签包括<style>、<script>、<a>、<svg>等。
rec_traverse()实现了对DOM树的深度优先遍历，在遍历过程中对不同类型的标签做删除或者合并处理，具体实现见算法5.1。在get_page_item()中调用，输入节点为soup的根节点。
算法 5.1  DOM树遍历
输入：node
输出：文本单元集合
伪代码：
children=node.children()  # node的子节点
text = ""  # 当前节点文本
FOR c: children:
	IF c为NavigableString类型:
	    text += c
	ELSE:  # c为Tag类型
	    IF c没有孩子节点:
		    IF c为block类型标签：
			    加入结果集
			ELSE:
			    text += c.string
		ELSE:  # 若有孩子节点
		    rec_traverse(c)  # 递归遍历孩子节点
IF text != "":
    加入结果集

fine_cut()方法对部分按照标签规则切分的文本进一步划分，划分依据为语言学中的句子分割符号，包括“。”、“！”、“\r\n”、“\n”等。
5.3.2 分词与实体识别
分词与实体识别实现了SentenceCutter类，能够输入一段文本，得到相应的分词后的结果，同时实现SentenceItem类，用于保存分词后的结果。SentenceCutter类定义见表 5-6，SentenceItem类的定义见表 5-7。
表 5-6 SentenceCutter类定义
名称	说明
sentence_item	SentenceItem类的实体
sentence	string，待处理的句子
get_sentence_item()	返回切分后的句子
ner()	实体识别与分词
reg_compare()	正则提取邮箱、电话、日期等
tidy()	合并ner()、reg_compare()的切分结果

表 5-7 SentenceItem类定义
名称	说明
word_seg	list，分词结果
token	list，实体标注结果

ner()实现了对实体的识别，其具体实现利用了哈尔滨工程大学开源的模型和Python组件pyltp。pyltp实体识别方法的输入为分词后的词组，在实现时使用jieba进行分词，并加载自定义机构词库，减少使用模型识别的错误。使用pyltp进行实体识别的效果如表 5-8所示，“O”表示非专名，NH表示机构名。
表 5-8 LTP实体识别举例
句实例	分词与实体识别结果
电子邮箱： ssmeng1999@163.com	电子邮箱/O :/O ssmeng1999/O @/NH 163.com/O
1994 － 1997 北京大学马克思主义学院 硕士	1994/O －/O 1997/O 北京大学/NH 马克思主义学院/O 硕士/O
2003-2008 人文与发展学院社科部 教师 副教授	2003-2008/O 人文/O 与/O 发展学院社科部/O 教师/O 副教授/O
对于句子中的邮箱、电话和日期，用正则匹配，以避免jieba在识别这类词语时出现错误，部分规则如表 5-9所示。
表 5-10所展示的是仅使用模型和使用规则与模型相结合在邮箱、日期、机构、电话、手机号实体类的准确率，表中的数据说明使用规则后实体的准确率有一定的提升。LTP模型与模型规则结合识别实例对比见表 5-11，其中EMAIL表示邮箱字符串，YY表示年份。
在使用模型和规则对一句话的实体识别结果会有着词语包含和交差的冲突，模块中实现tidy()方法处理多种方法标注的冲突，具体实现流程如图 5-2所示。






表 5-9特殊字符串规则匹配表
匹配项	实例	规则
邮箱	example@163.com	[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[A-Za-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?\.)+[A-Za-z0-9](?:[A-Za-z0-9-]*[A-Za-z0-9])?
年月日	2020年5月1日、
2020-5-1、
2020.5.1等	19\d{2}年\d{1,2}月\d{1,2}日|20\d{2}年\d{1,2}月\d{1,2}日|19\d{2}[-/。— 、.]{1,2}\d{1,2}[-/。— 、.]{1,2}\d{1,2}|20\d{2}[-/。— 、.]{1,2}\d{1,2}[-/。— 、.]{1,2}\d{1,2}
年月	2020年5月、
2020-5、
2020.5等	19\d{2}年\d{1,2}月|20\d{2}年\d{1,2}月|19\d{2}[-/。— 、.]{1,2}\d{1,2}|20\d{2}[-/。— 、.]{1,2}\d{1,2}
年份	2020年	19\d{2}年|20\d{2}年|19\d{2}|20\d{2}
手机号	13526358697	^1([358][0-9]|4[579]|66|7[0135678]|9[89])[0-9]{8}$
固定电话	010-76552782、
0752-27987495	\d{3}-\d{8}|\d{4}-\{7,8}

表 5-10 LTP模型与模型规则结合准确率对比
	邮箱	日期	机构名称	电话
LTP	0.8901	0.9714	0.9537	0.9312
LTP+规则	0.9812	0.9802	0.9813	0.9971

表 5-11 LTP模型与模型规则结合识别实例对比
LTP模型	LTP模型与规则结合
电子邮箱/O :/O ssmeng1999/O @/NH 163.com/O	电子邮箱/O :/O ssmeng1999@163.com/EMAIL
1994/O －/O 1997/O 北京大学/NH 马克思主义学院/O 硕士/O	19s94/YY－/X 1997/YY 北京大学/NH 马克思主义学院/O 硕士/O
2003-2008/O 人文/O 与/O 发展学院社科部/O 教师/O 副教授/O	2003/YY -/X 2008/YY 人文与发展学院社科部/NT 教师/O 副教授/O
5.3.3 文本单元分类
文本单元分类定义了SentenceClf类，将分块后的文本单元作为输入，提取特征，加载训练好的分类模型，预测得到相应的结果标签。SentenceClf类的具体定义如表 5-12所示。
文本单元分类任务使用的分类模型为训练好的XGBoost模型，词向量模型分为BERT预训练模型和基于Skip-Gram自训练的词向量模型。__init__()方法中实现了对模型的加载和对BERT服务的连接。对于BERT模型，使用平台的BERT服务获取词向量，模型选定为基础中文模型，维度为768。在此次应用场景下，Skip-Gram词向量模型的维度与仅用词向量表征文本之后分类的精准率（Precision）的关系如表 5-13向量维度与文本分类精准率关系所示，说明在此次文本分类场景中，维度为512能有更好的表征。在使用自训练词向量时，直接加载本地保存的词向量模型，并保存在vec_model中。


图 5-2 tidy()实现流程图

表 5-12 SentenceClf类定义
名称	说明
clf_model	文本分类模型
vec_model	自训练词向量模型
tf_idf_model	TF-IDF模型
bert_client	用于连接BERT服务器端
__init__()	初始化类属性，加载模型，连接BERT服务
get_label()	判断是否为主页
get_feature()	获取特征向量

表 5-13向量维度与文本分类精准率关系
维度	64	128	256	512	1024
Precision	0.864286	0.853858	0.8632266	0.8663240	0.8627100
R-Call	0.879032	0.864919	0.8729839	0.8770161	0.8729839
F1	0.867886	0.854808	0.8635755	0.8636954	0.8605291

此次分类任务涉及的特征有词向量特征，TF-IDF特征、实体数量特征以及位置特征。因此，在使用分类模型前，需要对不同的特征进行拼接。具体过程如下所示：
1、自定义词向量表征语义。对文本调用分词与实体识别组件，对分词后的文本做去除停用词处理，获取保留的词语词向量，所有词向量相加取平均即得到文本语句的向量，维度为512。
2、BERT表征语义。调用BERT服务需要引用BertClient模块，构造连接，即可进行语义表征，实现客户端代码，连接服务接口，调用相关函数并可以获得相应文本向量，维度为768。
3、提取TF-IDF特征。TF-IDF计算基于语料的词典，文本的TF-IDF特征向量维度与训练集的词典总数有关。在主页文本单元的训练语料下统计词库，去除文档率高于90%和文档频率低于10的词语，得到的词库词数为81501，相应的，文本的特征向量为81501维，TF-IDF表征文本得到的向量部分中部分词语对于的TF-IDF值如表 5-14样例文本部分词语的TF-IDF值所示。
表 5-14样例文本部分词语的TF-IDF值
文本单元	教授	姓名	硕士	学院
姓名 马冬梅	0.0	1.0	0.0	0.0
职称 教授	0.632	0.0	0.0	0.0
电子邮箱 [EMAIL]	0.0	0.0	0.0	0.0
[YY] [YY] 北京大学 马克思主义 学院 硕士	0.0	0.0	0.434	0.335
[YYMM] [YYMM] 哈尔滨师范大学 科技 哲学 专业 硕士	0.0	0.0	0.416	0.0

4、特征降维。TF-IDF特征向量为稀疏向量，同时TF-IDF模型词库中存在许多没有含义的字符串，需要对其做降维处理。本文选用PCA算法，实现时使用Sklearn工具中核PCA组件，其能执行复杂的非线性投影来降低维度，引入分类模型，通过最优化模型表现选择PCA方法最佳的核方法和超参数值。
5、特征拼接
文本向量是多个维度的特征的集合，各个特征特征值存在着不一致性。在特征向量拼接时，对单一类别的属性需要进行数据规范化操作，在一定程度上避免参数不平滑的问题。
6、文本分类
文本分类采用机器学习与启发式方法结合的方式，机器学习算法选用XGBoost，其关键参数和参数调优过程中的不分分参数取值表 5-16所示。
表 5-16 XGBoost参数调优
max_depth	3	5	7	3	5	7
min_child_weight	1	3	5	5	3	1
gamma	0.1	0.2	0	0.2	0	0.1
subsample	0.8	0.9	0.5	0.9	0.5	0.9
colsample_bytree	0.5	0.9	0.8	0.9	0.8	0.5
精准率	0.911	0.923	0.916	0.906	0.920	0.907
F1	0.891	0.907	0.897	0.890	0.903	0.899
5.3.4 冗余信息去除和数据集成
冗余消除 对于主页文本的分块、实体识别与文本单元分类后的数据，系统实现对文本冗余去除。维护表 4-7所示的规则，提取出系统需要的数据。
数据集成 完成冗余消除过程后，得到了姓名、职称职位、联系方式、教育经历、工作经历等字段值，需要将数据集成到平台的学者数据库中。数据入库之前需要对得到的数据进行检查。由于主页数据有数据获取模块从互联网中获取，存在着同名主页的可能性，系统遵循以下规则来判断抽取的最终结果是否符合预期：
1、维护学校主域名和学院主域名，判断主页链接的域名是否和预期机构域名相同。部分机构域名数据如表 5-17所示。
2、与数据库中已有的学者数据比较数据相似度，对不容易变化的学者标签，根据编辑距离计算文本相似度，若数据的平均符合相似度大于阈值0.8，则更新原有数据，否则新增学者记录。
表 5-17学院机构域名数据对应表
学校名称	学院名称	域名
东南大学	软件学院	cose.seu.edu.cn
东南大学	化学化工学院	chem.seu.edu.cn
东南大学	材料科学与工程学院	smse.seu.edu.cn
北京大学	化学与分子工程学院	chem.pku.edu.cn
南京大学	计算机科学与技术系	cs.nju.edu.cn
5.4 术语抽取模块实现
5.4.1 数据接口
术语抽取模块处理的数据来源于三螺旋数据服务平台的学术论文，实现了DocDownloader类，连接平台的重名消歧子系统服务，获取论文数据。
表 5-18 DocDownloader类定义
名称	说明
start_year	int，起始年份年份
client	连接重名消歧服务端
__init__()	完成了重名消歧服务端的连接
download()	存储论文数据
doc_filter()	过滤论文数据

__init__()方法中实现了DocDownloader的初始化，完成了重名消歧服务端的连接。download()方法中实现向重名消歧服务端请求数据，调用doc_filter()方法，过滤掉已经处理过的论文，并将新论文按照学科存储在本地文件中。为了实现论文的过滤，本地维护一个存储论文MD5编码的数据表，并按MD5编码建立索引，在从服务端获取论文数据之后，将论文的MD5编码与本地的进行比较，从而判断是否保留论文。
5.4.2 术语抽取器
术语抽取部分定义了术语抽取器类TermExtractor，实现输入系列文档，完成文档中的候选术语相关的统计量和C-Value的计算，并存储。类具体定义如图所示。
表 5-19 TermExtractor类定义
名称	说明
conn	连接文档数据库
reg_dir	string，规则文件目录
__init__()	初始化
get_c_value()	计算C-Value
get_mi()	计算MI
word_stat()	统计字符串相关概率
load_words()	加载本地文档数据库候选术语数据
update_words()	将新增候选术语数据更新到文档数据库
load_base()	加载基础词库
load_reg()	加载词性组成规则
__init__()方法中完成TermExtractor初始化，完成本地数据的连接，以及词性规则文件和基础术语词库目录的指定。load_words()方法实现连接本地数据库，加载已有的候选术语数据。update_words()将新增的候选术语数据更新到本地的MongoDB数据库，更新时，对于新词，需要将其有意义的子串的母串集合数s_nums增加1。load_base()方法实现加载基础术语词库，在分词时作为自定义词典载入。load_reg()实现加载某一学科的词性组成规则。其中基础术语词库和词性组成规则都以学科为单位存储在本地文件中。word_stat()方法实现了对某一学科论文集中文本的分词、按词性规则组合新词、统计词频和母串频率的功能，具体实现过程如图所示。
get_c_value()方法实现了对词语C-Value值的计算，并以学科为单位存储到文件中，具体实现过程如图所示。
算法 C-Value计算流程
输入：本地文档数据库候选术语数据items，items的结构与数据库设计一致；学科文档分布docs_num；学科代码dis_code
输出：{word: c_value}
算法流程：
words = dict()
FOREACH item IN items:
word = item[“Word”]
total = sum(docs_num.values)  # 总文档数
# 计算word总的文档频率
df = 0
FOREACH key, value IN item[“Data”]:
    df += value[“df”]
# 计算C-Value
value = item[“Data”][dis_code]
dis_code = key
f = value[“f”]
fs = value[“fs”]
s_nums = value[“s_nums”]
M = docs_num[dis_code]  # 学科dis_code文档数
N = total - M # 背景语料文档总数
IF fs == 0:  # 若word为极大
    C_Value = len(word)*f*N/df/M
ELSE:  # 若word不为极大串
    C_Value = len(word)*(f-fs/s_nums)*N/df/M
    words.append((word, C_value))  # 加入结果集
RETURN  words  # 返回结果集

get_mi()方法实现了对词语MI值的计算，具体流程如图所示。
算法 MI值计算
输入：本地文档数据库候选术语数据items，items的结构与数据库设计一致；学科代码dis_code
输出：{word: c_value}
算法流程：
words = dict()
FOREACH item IN items:
word = item[“Word”]
total = sum(docs_num.values)  # 总文档数
# 计算word总的文档频率
df = 0
value = item[“Data”][dis_code]
words[word] = value[“f”]

# 计算MI
FOREACH key, value IN words:
FOREACH w in key的子串：
    IF w IN words:  # 若子串为符合规则的候选术语
        words[“words”] = 1.0*words[“words”]/words[“w”]
RETURN words
为了解决语料库较大时，内存占用过大的问题，定义了PathLineSentences类直接从磁盘得到论文文本的可迭代对象。具体定义如图所示。其中source为指定的论文所在的文件目录名，input_files为source目录下的所有文件的名称，max_sentence_length为一句话中的最大词数，系统定义为10000，limit为读取的文本总数，设置为空时，读取文件夹下的所有文件。
表 5-20 PathLineSentences类定义
名称	说明
source	string，文件目录
input_files	list，文件名列表
max_sentence_length	int，一句话中的最大词数
limit	int，读取前limit大小的句子
__init__()	初始化
__iter__()	实现PathLineSentences对象可迭代

__init__()的输入参数为文件目录名或者文件名已经读取的句子数，具体实现如图：

图 5-3 PathLineSentences对象初始化
同时，实现__iter__()方法，使PathLineSentences的实例为可迭代对象，具体实现如图所示。

图 5-4 实现可迭代
5.4.3 主程序控制
主程序term_extractor.py控制着术语抽取模块其他组件的运行，在主程序中组件的调用流程如图所示。

图 5-5术语抽取控制程序执行流程
5.5 应用服务模块实现
应用服务模块通过实现HTTP接口响应用户请求，其整体处理流程图如图所示。

图 5-6应用服务模块处理流程
5.5.1 模块项目结构介绍
模块的实现基于flask和REST插件flask-restful，本模块的项目结构如图所示。其中，api_中定义了路由和相关处理函数；common中定义了通用的处理函数，包括登录验证、请求消息体处理、响应消息体处理、响应消息状态码以及返回的数据结构；db中实现了数据库的链接、对数据表的基础操；services中封装了通用的业务逻辑、与数据层的交互、连接远程服务；config.init数据库配置文件中的端口信息和数据库名称信息；config.py实现了读取配置文件中的信息；start.py实现了对应用的初始化，并启动应用。

图 5-7应用服务模块项目文件结构
5.5.2 请求参数处理
common.request_process.py实现了请求参数的通用处理方法，具体定义如图所示。其中clean_none()方法实现去除请求值为空的参数，verify_all_param_must()验证是否包含所有必传参数，verify_all_param_type()用于验证参数类型是否正确。
表 5-21 RequestProcess类定义
名称	说明
clean_none()	期初值为空的参数
verify_all_param_must()	验证是否包含必传参数
verify_all_param_type()	验证参数类型
5.5.3 返回消息处理
网络中传输的数据通常为JSON格式或XML格式，model2json.py自定义编码类MyEncoder，实现了将学者数据转化为JSON格式。具体实现如图所示，其中Scholar、EduExp、 WorkExp、Contact、TermCValue、Term为自定义数据类型。

图 5-8 Python对象序列化实现
同时，为了更好的处理获取的学者数据与术语数据，模块定义了响应消息类。包括响应消息基类、学者基本信息模型ScholarResp类、联系方式数据模型Contact类、教育经历数据模型EduExp类以及工作经历数据模型WorkExp类、包含C-Value的术语数据模型TermCValueResp类以及术语词典数据TermResp类，类之间的关系如图所示。

图 5-9 消息响应相关类类关系图
5.6 本章小结
本章主要针对面向学者画像的信息系统的进行实现。首先，介绍了本系统的开发环境；然后，对系统的数据获取与预处理模块、主页抽取模块、术语抽取模块和应用服务模块的实现进行了详细描述。
 
第六章 系统测试与性能分析
6.1 单元测试
本小结根据不同模块的功能设计不同的测试用例，对各个模块进行单元测试，以发现该模块的实际结果出现不符合预期的情况。
6.1.1 数据获取与预处理模块
采集器组件测试用例如表 6-1所示。自定义待获取主页的学者名单和起始时间参数，启动模块，读取学者名单，开始获取与预处理工作。通过分析日志，得到获取链接总数和采集到的主页总数。
表 6-1采集器组件测试用例
用例名称	采集器组件测试
测试目的	（1）验证采集器是否能够成功拼接请求连接
（2）验证是否能够成功调用主页链接识别组件
（3）验证是否能够成功采集学者主页
测试用例前置条件	（1）自定义待获取学者名单数据包含学者姓名、所在学校和学院
（2）百度搜索结果页面解析规则未改变
用例设计	（1）自定义测试名单包含不同学校和学院的学者姓名20人
（2）配置采集器参数起始时间为“2015”
预期结果	（1）成功拼接请求连接
（2）成功调用主页链接识别组件
（3）成功采集学者主页，满足系统需求
实际结果	（1）采集到学者主页21条
（2）成功调用学者主页链接识别组件，成功识别出21条主页链接
（3）总共耗时130s，合集请求41次
状态	通过
主页链接识别组件测试用例如表所示。自定义搜索结果数据集，包括链接、标题、摘要，共100条，作为主页链接识别组件的输入，最终识别出的主页链接数为11条，未识别出的主页链接为1条。
表 6-2主页链接识别组件测试用例
用例名称	主页链接识别组件测试
测试目的	（1）验证组件是否能够成功接受参数
（2）验证是否能够对不正确的参数进行处理
（3）验证是否能够成功返回识别后的结果
测试用例前置条件	（1）百度搜索结果页面显示的相关数据未改变
用例设计	自定义测试主页链接和非主页链接共100条数据，每条数据包括链接、标题和百度搜索页相关摘要。
预期结果	（1）组件成功接受参数
（2）正确处理对不符合要求的参数
（3）成功返回识别后的结果
实际结果	（1）正确处理对不符合要求的参数
（2）识别的召回率为0.96，准确率为0.96
（3）成功返回识别结果
状态	通过
预处理组件测试用例如表所示，调用预处理组件对收集到的主页数据进行对其编码、字体、文件格式进行统一。同时，对电话号码、日期等特殊字符串进行规整处理。测试数据共100条，成功处理70条。
表 6-3预处理组件测试用例
用例名称	预处理组件测试
测试目的	（1）验证组件是否能够成功接受参数
（2）验证是否能够对不正确的参数进行处理
（3）验证是否能够成功对部分需要预处理的文件进行处理
测试用例前置条件	主页数据采集完成
用例设计	自定义主页HTML测试文件100条
预期结果	（1）组件成功接受参数
（2）正确处理对不符合要求的参数
（3）成功对文件的编码、字体、文件格式进行统一，对文本中的电话号码、日期等特殊字符串进行规整处理。
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）测试数据共100条，成功处理70条。
状态	通过
6.1.2 主页抽取模块
文本分块组件测试用例如表所示，组件对收集到的主页数据进行分块处理。测试数据共50条。
表 6-4文本分块组件测试用例
用例名称	文本分块组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功构建DOM树
（3）验证是否能够正确去除标签、主页HTML进行分块处理
测试用例前置条件	主页数据采集和主页预处理完成
用例设计	自定义测试不同学校不同学院的学者主页HTML文档50条
预期结果	（1）组件成功接受参数、正确处理对不符合要求的参数
（2）成功构建DOM树，去除标签；对主页HTML进行分块，得到文本单元
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）测试数据共50条，得到文本单元数1000条。
状态	通过
分词与实体识别组件测试用例如表所示，组件对分块后的文本单元进行分词与实体识别。测试数据共300条，识别出的实体共，未识别出的实体，准确率，满足系统要求。
表 6-5分词与实体识别组件测试用例
用例名称	分词与实体识别组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功对文本分块得到的文本单元进行分词和实体识别处理
测试用例前置条件	主页分块完成
用例设计	自定义测试不同学校不同学院的10个学者主页HTML文档文本分块的文本单元，共300条。
预期结果	（1）组件成功接受参数
（2）正确处理对不符合要求的参数
（3）成功对文本分块得到的文本单元进行分词和实体识别处理
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）识别出的实体共，未识别出的实体，满足系统要求
状态	通过

文本单元分类组件测试用例如表所示，调用分词和实体识别组件，得到有实体标注的分词列表，对文本单元提取特征，进行分类，得到文本单元的类别信息。测试数据共300条，成功处理300条。
表 6-6文本单元分类组件测试用例
用例名称	文本单元分类组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功调用分词与实体识别组件
（3）验证是否能够成功提取特征，经分类模型得到文本单元的类别
测试用例前置条件	主页分块和实体识别完成
用例设计	自定义测试不同学校不同学院的10个学者主页HTML文档文本分块的文本单元
预期结果	（1）成功接受参数、对不正确的参数进行处理
（2）成功调用分词与实体识别组件，得到文本单元的分词和实体标注结果
（3）成功获得文本单元的类别
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）10个学者主页HTML文档文本经分块得到的文本单元共1000条，组件成功返回分类结果
（3）分类结果的准确率为，符合系统要求
状态	通过
冗余去除组件测试用例如表所示，调用组件对已经分类的文本单元进行冗余去除。准备文本单元分类组件的测试数据共100条，成功处理100条。最终，提取出190条符合要求的名词，名词检查过滤20条，最后入库准确率100%，符合系统要求。
表 6-7冗余去除组件测试用例
用例名称	冗余去除组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功提取文本单元中的有效信息
测试用例前置条件	文本单元分类完成
用例设计	自定义主页数据10条，经文本分块组件、实体识别组件和文本单元分类组件之后得到的文本单元数据100条。
预期结果	（1）组件成功接受参数
（2）正确处理对不符合要求的参数
（3）成功对文本单元中的实体名词和专有名词进行提取，并对提取的名词进行检查
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）测试数据共200条，提取出190条，名词检查过滤20条，最后入库准确率100%，符合系统要求。
状态	通过
6.1.3 术语抽取模块
数据接口测试用例如表所示，通过调用重名消歧系统，获取论文数据，并在本地对重复的论文数据进行过滤。
表 6-8数据接口组件测试用例
用例名称	数据接口组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功调用重名消歧系统
（3）验证是否能够对历史处理过的论文数据进行过滤，将没有处理过的论文保存在本地文档数据库中
（4）验证是否能够成功维护记录论文标志的数据库
测试用例前置条件	论文数据处理完成
用例设计	自定义测试参数：学科代码（0812）、起始时间（2018）
预期结果	（1）组件成功接受参数、正确处理对不符合要求的参数
（2）成功调用重名消歧系统
（3）成功对历史处理过的论文数据进行过滤，并维护用于论文去重的数据库
（4）成功将论文保存在本地文档数据库中
实际结果	（1）组件成功接受参数，并正确处理对不符合要求的参数
（2）共获取1580篇论文，过滤423篇，保存本地文档数据库1157篇
状态	通过
术语抽取器测试用例如表所示，设计用于测试的0812学科的论文集，对该集合中的术语进行C-Value计算，并更新存储。
表 6-9术语抽取器测试用例
用例名称	术语抽取器测试
测试目的	验证是否能够成功读取文档数据库中存储的论文文本，并对论文进行术语抽取
测试用例前置条件	数据接口获取论文完成
用例设计	自定义数据接口获取的1157篇论文为测试数据集
预期结果	成功完成候选术语的词频统计与C-Value计算
实际结果	成功处理新增论文1157篇，新增候选术语10个，更新术语382个
状态	通过

6.1.4 应用服务模块
应用服务模块测试用例如表所示，自定义模拟数据，包括学者数据、术语抽结果数据、术语词典等，通过设置不同参数，请求应用服务模块，查看返回结果是否符合预期。
表 6-10应用服务模块测试用例
用例名称	抽取结果数据接口组件测试
测试目的	（1）验证组件是否能够成功接受参数、对不正确的参数进行处理
（2）验证是否能够成功对主页HTML进行分块处理
测试用例前置条件	抽取服务接口定义完成、数据库服务接口定义完成
用例设计	自定义模拟数据，设置不同的请求参数
预期结果	（1）成功接受参数，正确处理对不符合要求的参数
（2）完成对应服务逻辑，返回结果符合预期
实际结果	符合预期
状态	通过
6.2 系统功能测试
系统功能测试作用在于测试本系统是否能够完成设计阶段定义的功能，本系统功能测试分为数据接口功能测试和后台管理功能测试。
6.2.1 数据接口功能测试
数据接口功能，是指系统提供的通过配置参数发送Http请求的方式，以获取响应的抽取结果数据供用户使用。测试数据的部分设计如表所示。
表 6-11结果数据接口功能测试部分用例
用例编号	功能描述	参数示例	预期结果	状态
1	按学校查询学者数据	{"school_name": "东南大学"}	返回学者数据表中东南大学的学者数据	通过
2	按学校和学院查询学者数据	{"school_name": "东南大学", "ins_name": "软件学院"}	返回学者数据表中东南大学软件学院的学者数据	通过
3	按学科获取学术术语词库	{"discipline_code": "0812"}	返回“0812”学科的学术术语词库数据	通过
4	按学科获取学者术语C-Value数据	{"discipline_code": "0812", "c_value": 1}	返回“0812”学科的学术术语C-Value数据	通过
对于前端请求“localhost:8080/scholar/list?school_name=东南大学&offset=0&limit=1”学者数据返回示例如图所示。

图 6-1学者数据接口返回示例
6.2.2 后台管理功能测试
后台管理功能包括术语数据管理、学者数据管理、术语抽取服务管理和学者抽取服务管理，针对后台管理功能设计不同的测试用例，部分用例如表所示。
表 6-12后台管理功能测试部分用例
用例编号	功能描述	参数示例	预期结果	状态
1	查询学者数据	{"status": 1, "offset": 0, "limit": 5 }	返回学者数据表中东南大学的学者数据	通过
2	查询术语词典	{"discipline_code": "", "offset": 0, "limit": 20}	返回术语词库中前20词	通过
3	查询术语C-Value	{"discipline_code": "0812", "offset": 0, "limit": 20}	返回“0812”学科的学术术语词库数据	通过
4	按学科获取学者术语C-Value数据	{"discipline_code": "0812", "c_value": 1,"offset": 0, "limit": 20}	返回“0812”学科的学术术语C-Value数据排名前20的词语	通过
5	开启抽取抽取任务	{ "name": "", "school": "东南大学", "ins": "软件学院", "start_yymm": "1560529477" }	更新东南大学软件学院的学者数据	通过
6	开启术语抽取任务	{ "dis_code": "0812", "start_yy": "1420041600" }	更新0812学科的术语数据	通过

图 6-2学者数据展示

图 6-3学者画像数据展示

图 6-4术语词典数据展示

图 6-5术语抽结果数据查询

图 6-6主页抽取参数设置

图 6-7主页抽取参数设置
6.3 系统评测
本小结主要通过描述实验数据、评价方法和实验结果分析来评价系统所使用的主要抽取策略和算法。
6.3.1 数据准备
本系统的实验建立在自建的数据集上，为了能够获得较为全面的试验结果，不同的试验任务中构建不同的数据集时。在主页抽取实验中，分别构建主页链接识别数据集、词向量训练数据集和文本单元分类数据集；在术语抽取实验中，构建包含多个学科领域的论文文本，具体数据集说明如表所示。
表 6-13实验数据集说明
试验名称	数据集说明
主页链接识别	非主页数据16343，主页数据32141，其中每项数据包括链接、页面标题和页面摘要。
文本分句	随机选取学者主页的HTML文档100个。
词向量训练	选取不同学校的42086个学者主页进行分词，得到2185万词。
文本单元分类	选取不同学校不同学院的400个主页进行分块，得到7406条数据，具体数据分布如表 6-14所示。
术语抽取	选取学科大类“工科”下不同学科的论文，学科与对应的论文数量如表 6-15所示。

表 6-14文本单元数据集分布
文本标签	姓名	性别	出生年月	电话	邮箱	学历	职称职位	教育经历	工作经历	出版信息	多信息	无关文本
数量	433	10	6	20	64	89	137	172	76	1363	136	4816

表 6-15术语抽取测试集部分学科论文数量
学科代码	力学	机械
工程	材料科学与工程	电气
工程	电子科学与技术	计算机科学与技术	环境科学与工程	软件
工程
学科	0801	0802	0805	0808	0809	0812	0830	0835
数量	112580	143502	129524	116669	118716	148286	91060	79443
6.3.2 主页链接识别
（1）主页链接识别分析
本系统通过设置搜索引擎搜索关键词，得到搜索结果，以搜索结果信息为依据识别出主页链接，将此策略与以高校机构官方站点为根节点爬取站点所有网页的策略比较。表 6-16展示了实验过程中随机选取10个高校学院的网站相关数据，其中站点链接数表示爬虫以网站根节点出发遍历的深度最大为3的链接总数，主页链接数为每个站点所包含的主页链接总数。
实验设计了前文所述的两种不同的主页获取策略，最终数据的统计结果如表 6-17，其中有效主页率为识别为主页的链接中有效主页链接的占比，爬虫请求主页命中率为爬虫请求的链接中有效主页链接的占比。从实验结果可知，本系统采用的基于搜索引擎的策略保证主页召回率和准确率的基础上，主页命中率远远高于基于站点根目录的策略，这说明本系统的主页收集模块需要请求的网页链接更少，效率更高。
表 6-16高校学院网站相关数据
学校	学院	主页链接数	站点链接数
大连理工大学	水利工程学院	111	172
湖南大学	电气与信息工程学院	279	1192
厦门大学	材料学院	81	1143
华中科技大学	机械科学与工程学院	230	1220
兰州大学	物理科学与技术学院	163	1450
北京师范大学	生命科学院	104	193
北京航空航天大学	自动化科学与电气工程学院	191	910
清华大学	建筑学院	118	649
南京大学	计算机科学与技术系	195	1047
哈尔滨工业大学	理学院	83	344
表 6-17主页链接识别对比实验
方法	主页命中率（%）	有效主页率（%）	主页召回率（%）
基于搜索引擎	98.42	98.73	95.29
基于站点根目录	18.69	98.70	96.11
6.3.3 主页抽取
（2）文本分句结果分析
文本分句的目的是去除主页HTML文档中的标签和注释等无用信息，将其余文字划分成多个的文本单元，而文本单元跟多的事希望包含的信息尽量单一，以便于后续的文本分类和规则提取工作。本文随机抽取100个爬取的主页，对其进行文本分句实验。

图 6-8文本分句结果统计

图 6-8为文本分句结果中“无关信息文本”、“单信息文本”以及“多信息文本”的占比。在文本分句的结果中，“单信息文本”和“多信息文本”是提取学者信息的关键文本，约占所有文本结果的35%；而需要进一步处理的“多信息文本”约占所有文本结果的2%，占比较小，符合系统要求。
（3）文本单元分类结果分析
本系统文本单元分类过程中，提取了多种不同含义的特征，融合后作为分类模型的输入，分类模型采用XGBoost。图 6-9展示了在分类模型输入分别为TF-IDF特征、自训练词向量Word2Vec特征，预训练词向量特征，以及特征融合、结合规则抽取时的分类结果的比较。

图 6-9文本单元分类结果比较
如图 6-9所示，TF-IDF特征和词向量特征相比，词向量特征具有更高的准确率与召回率，这说明，词向量在表征文本上由于TF-IDF；特征融合后，结果明显由于只采取一种特征时的结果，说明了特征融合的有效性；加入规则，进一步让结果得到提升，这一方面是因为，训练集中部分文本单元非常少，存在数据不平衡的问题，在分类模型中过滤掉了短句内容较少，机器学习的算法难以习得该类型的特征，部分过少的数据也会被分类模型当做无效数据去除，例如“出生年月”、“学历”和“电话”的文本；另一方面是因为，部分文本单元的词语数较少，难以用TF-IDF和词向量很好的表征文本，如部分描述职称的文本，只有两个词。
图 6-10、图 6-11分别为特征融合和结合规则方法的分类结果中准确率和召回率的比较。图中可以看到，在加入规则后，类别“出生年月”、“电话”、“职称”的准确率和召回率有了显著的提升，“学历”、“教育经历”、“工作经历”也有小幅度的提升。

图 6-10特征融合和结合规则方法的分类的准确率比较

图 6-11特征融合和结合规则方法的分类的召回率比较

（4）抽取结果分析
描述学者画像的主页抽取任务中，最终的结果为一系列实体名词与专业名词，为了进一步说明系统所采用的抽取策略的有效性，使用基于规则方法的提取结果进行对比试验。规则方法是基于本文文本单元分类场景的数据手工定义的一套规则集合。这两种方法在测试时使用同样的测试集，分类的结果中各个字段的准确率与召回率的比较如表 6-18所示。实验结果表明，本系统的文本分类方法在绝大多数类别上，效果超过了仅使用规则方法的效果，同时系统的平均准确率达到了九成，文本单元分类的结果可以被可靠的运用于之后的字段提取中。不过仍有少数字段，本系统的方法与规则提取的方法差别不大，例如出生年月，这主要是因为这些文本单元内容较少，且具有很强的规则性，基于规则的提取在这一类文本单元上表现较好。


表 6-18主页抽取结果比较
标签	基于规则提取（P/R）	系统抽取策略（P/R）
姓名	0.813323	0.841220	0.891579	0.876542
性别	1	0.996257	1	0.996257
出生年月	0.871293	0.862519	0.870981	0.862519
电话	0.970612	0.997129	0.970612	0.997129
邮箱	0.981827	0.981092	0.981827	0.981092
学历	0.836511	0.840981	0.902697	0.881029
职称职位	0.852763	0.852763	0.852763	0.852763
教育经历	0.792827	0.812340	0.971239	0.971239
工作经历	0.810383	0.790012	0.923918	0.923918
6.3.4 术语抽取
（5）术语抽取结果分析
术语抽取过程中涉及的指标有互信息和C-Value*，指标的阈值选择通过观察分别确定MImin为20和CVmin为3.4。
术语抽取的结果为某个学科下词语某个维度的权值排序，本文采用改进的C-Value*来表示词语的权重，并与同样基于词频统计的术语抽取方法TF-IDF、C-Value进行比较，并用mAP评价方法具体说明。表 6-19展示了不同权重计算下的排序结果mAP值的比较，其中C-Value*取得了比其他方法更好效果，验证了方法的有效性。
表 6-19术语抽取结果比较
方法	mAP
TF-IDF	0.256351
C-Value	0.375000
C-Value*	0.423636

表 6-20召回率R与Top-K、准确率P的关系
R	0.1	0.2	0.3	0.4	0.5	0.6	0.7	0.8	0.9	1.0
Top-K	322	664	1066	1448	1790	2172	2554	1997	3500	4063
P	0.94	0.91	0.87	0.85	0.83	0.82	0.82	0.79	0.76	0.73
表 6-20展示了C-Value*得到的排序结果中，召回率分别设置为0.1~1.0时Top-K的取值以及准确率的结果。由表 6-20中的数据可知，当Top-K为664时，准确率达到0.91，当Top-K为1448时，准确率达到0.85，符合系统对术语抽取结果的要求。
表 6-21为术语抽取结果中长术语的术语C-Value*排序的部分结果。由排序结果可知靠前的长术语均是对计算机科学与技术学科领域内的核心知识表述，具有很强的专业性；排名靠后的可以明显判断出其不属于术语范畴，大都是满足一定词性规则所遗留下来的短语结构。而C-Value*能将这两种不同的词语词组很好的识别出来语的正确识别，保障了整个术语库的质量，体现出领域知识特点。
表 6-21部分候选长术语C-Value*的比较
词组性长术语	词频	C-Value*权重
Web 服务	486	9.923060483
无线 传感器 网络	297	9.795984973
传感器 网络	435	9.762879591
无线 传感器	337	9.394029098
入侵 检测	329	9.359196662
... ...
案例 分析	13	4.629726067
解决 用户	13	4.629579872
统计分析 方法	14	4.629432326
不 超过	13	4.629016571
反馈 机制	13	4.628849417
6.4 本章小结
本章主要信息抽取系统进行系统测试与性能分析。首先，对系统的各个模块进行单元测试；然后，对系统的数据接口功能和后台管理功能进行了功能测试；最后对信息抽取系统所采用的方法和策略进行了分析和比较，说明了系统的有效性与实用性。
 
第七章 总结与展望
7.1 总结
本文围绕面向学者画像的信息抽取系统来展开叙述。首先，介绍了系统的研究背景及意义，阐述了相关领域的研究现状；然后，介绍了系统涉及到的相关研究和技术；最后，在前面的基础上，提出来适用与产学研背景的用于描述学者画像的信息抽取系统。该系统以学者画像在产学研背景应用的角度出发，完成了学者主页数据的采集功能、主页抽取功能、信息抽取功能和应用服务功能。论文的主要工作体现在以下三个方面：
（1）提出了学者主页发现采集和识别的方法
本文采用基于搜索引擎的学者主页发现策略。首先，通过设置搜索参数得到搜索结果；然后，在排名靠前的结果中识别出主页链接，得到主页数据。第六章的实验分析表明，该策略在保证准确率的条件下，提升了数据收集的效率。
（2）提出了一种实用的学者主页抽取方法
本文采用规则与机器学习相结合的方法对主页进行抽取。首先，提出了一种基于DOM的文本分句算法，对主页HTML文档进行分句，得到文本单元；然后，为了准确地文本中的实体，提出了自定义词典、规则和实体识别工具相结合的策略；最后，通过提取文本单元的统计特征和语义特征，对文本单元进行分类，并对文本单元中的实体和专有名词进行提取，得到最终的结果。在真实的主页数据集上测试的结果表明，此抽取策略能够全面而准确抽取出主页中的学者画像属性。
（3）提出了基于学术文献数据的学科领域术语抽取
本文采用C-Value和互信息为重要抽取指标，对学术文献数据中的词语进行指标计算，通过设定的阈值过滤不满足要求的词语，最终得到术语抽取的结果。为了扩展新术语，本文还提出了一种词组的词性规则自动生成的算法，以学科为单位，计算同一个学科内的术语组成规则，将一些不符合通用规则的术语也纳入词语度的计算，实验数据表明，词性规则自动生成算法的使用提升了术语的抽取率。
（4）实现了一种面向学者画像的信息抽取系统
本文通过分析产学研背景下的学者画像模型，定义抽取目标范围，设计抽取策略，实现了信息抽取系统。系统功能包括数据获取与预处理功能、主页抽取功能、术语抽取功能、后台管理功能以及数据接口功能。本文对信息抽取系统也做了功能测试，系统能够有效地完成主页抽取和术语抽取任务，抽取的结果能够被三螺旋数据平台直接利用，验证了系统的可用性与有效性。
7.2 展望
本文实现的信息抽取系统，在满足描述学者画像的基础功能外，还存在着一些不足之处，之后的研究工作可以从以下几个方面改进：
（1）英文学者主页的处理。由于前期数据获取的英文学者主页数量少，难以构建数据集用于算法试验，对英文主页的抽取不足，今后可以考虑编写获取英文主页的爬虫，构建更完整的数据集。
（2）抽取数据源的限制。本次课题研究的对象是产学研背景下的学者画像的抽取，抽取了学者画像中的基本信息和表示研究方向的术语数据。若要得到更完整的学者画像，需要获取来源的数据，例如收集学者的国家奖项信息、重大专项项目数据等。
（3）术语抽取数据集的构建。本次课题用于术语抽取研究的数据集没有经过手工标注术语词，从而采用了基于统计的术语抽取方法。未比较使用有监督学习的方法的效果。之后可以通过构建有术语标注的数据集，尝试更多的方法，对术语抽取策略进行改进。
 
参考文献
[1]	Tang J , Zhang J , Zhang D , et al. ArnetMiner: An Expertise Oriented Search System for Web Community.[C]// International Conference on Semantic Web Challenge. CEUR-WS.org, 2007.
[2]	Tan Z , Liu C , Mao Y , et al. AceMap: A Novel Approach towards Displaying Relationship among Academic Literatures[C]// the 25th International Conference Companion. International World Wide Web Conferences Steering Committee, 2016.
[3]	林燕霞, 谢湘生. 基于社会认同理论的微博群体用户画像[J]. 情报理论与实践, 2018, 041(003):142-148.
[4]	Schrammel J , Kffel C , Tscheligi M . Personality traits, usage patterns and information disclosure in online communities[C]// Proceedings of the 2009 British Computer Society Conference on Human-Computer Interaction, BCS-HCI 2009, Cambridge, United Kingdom, 1-5 September 2009. 2009.
[5]	Qiu L , Leung K Y , Jun Hao H O , et al. Understanding the psychological motives behind microblogging.[J]. Studies in Health Technology & Informatics, 2010, 154:140.
[6]	刘海鸥, 孙晶晶, 苏妍嫄, et al. 国内外用户画像研究综述[J]. 情报理论与实践, 2018, 041(011):155-160.
[7]	袁莎, 唐杰, 顾晓韬. 开放互联网中的学者画像技术综述[J]. 计算机研究与发展, 2018, 55(09):1903-1919.
[8]	Sager N. Natural Language Information Processing: A Computer Grammmar of English and Its Applications[M]. Addison-Wesley Longman Publishing Co. Inc. 1981.
[9]	Dejong G F. An overview of the FRUMP system[M]// Strategies for Natural Language Processing. 1982:149-176.Bay Y M. Development and applications of patent map in Korean high-tech industry[C]//The first Asia-Pacific conference on patent maps, Taipei, October. 2003, 29: 3-23.
[10]	Kim J T, Dan I M. Acquisition of Linguistic Patterns for Knowledge-Based Information Extraction[J]. IEEE Transactions on Knowledge & Data Engineering, 1995, 7(5):713-724.
[11]	Riloff E. Automatically constructing a dictionary for information extraction tasks[C]// Eleventh National Conference on Artificial Intelligence. AAAI Press, 1993:811-816.
[12]	Soderland S, Fisher D, Aseltine J, et al. CRYSTAL: Inducing a Conceptual Dictionary[J]. Proc.int’l Joint Conf.artificial Intelligence, 1995, 2(3):N12.
[13]	石倩, 陈荣, 鲁明羽. 基于规则归纳的信息抽取系统实现[J]. 计算机工程与应用, 2008, 44(021):166-170.
[14]	乔磊, 李存华, 仲兆满,等. 基于规则的人物信息抽取算法的研究[J]. 南京师大学报:自然科学版, 2012, 35(004):134-139.
[15]	Frantzi K T , Ananiadou S , Mima H . Automatic Recognition of Multi-word Terms: The C-value/ NC-value Method[C]// International Conference on Theory and Practice of Digital Libraries. 2000.
[16]	Chang C H , Kayed M , Girgis M R , et al. A Survey of Web Information Extraction Systems[J]. IEEE Transactions on Knowledge & Data Engineering, 2006, 18(10):1411-1428.
[17]	鞠久朋, 张伟伟, 宁建军,等. CRF与规则相结合的地理空间命名实体识别[J]. 计算机工程, 2011(07):210-212.
[18]	S Gupta ，G Kaiser ，D Neistadt: DOM-based Content Extraction of HTML Documents[C]
[19]	孙承杰, 关毅. 基于统计的网页正文信息抽取方法的研究[J]. 中文信息学报, 2004, 18(5).
[20]	Ling H , Long C . Web information extraction based on visual block segmentation[J]. Journal of Computer Applications, 2008.
[21]	Li X , Zhang W , Wang D , et al. Algorithm of web page similarity comparison based on visual block[J]. Computer ence and Information Systems, 2019, 16:28-28.
[22]	Cai D, Yu S, Wen J R, et al. VIPS: a Vision-based Page Segmentation Algorithm[J]. Microsoft Research, 2003.
[23]	李朝, 彭宏, 叶苏南,等. 基于DOM树的可适应性Web信息抽取[J]. 计算机科学, 2009, 36(7):202-203.
[24]	Wang J, Lochovsky F H. Data-rich Section Extraction from HTML pages[C]// International Conference on Web Information Systems Engineering. IEEE, 2003:313-322.
[25]	Liu B, Grossman R, Zhai Y. Mining data records in Web pages[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, Dc, Usa, August. DBLP, 2003:601-606.
[26]	Wu M, Marian A. Corroborating Answers from Multiple Web Sources[C]// Tenth International Workshop on the Web and Databases, WEBDB 2007, Beijing, China, June. DBLP, 2007:431-449.
[27]	Yang S Y , Hsu C L . Ontology-Supported Web Recommender for Scholar Information[C]// International Conference on Hybrid Intelligent Systems. IEEE, 2009.
[28]	Brewster, Christopher & Iria, J. & Zhang, Ziqi & Ciravegna, Fabio & Guthrie, Louise & Wilks, Yorick. (2007). Dynamic Iterative Ontology Learning.
[29]	Wolf, Petra & Bernardi, Ulrike & Federmann, Christian & Hunsicker, Sabine & Stuhlsatzenhausweg, Dfki. (2011). From Statistical Term Extraction to Hybrid Machine Translation.
[30]	Liang Y H , Li J X , Ye L , et al. The Chinese Unknown Term Translation Mining with Supervised Candidate Term Extraction Strategy[J]. Procedia Engineering, 2011, 15(none):1388-1392.
[31]	Pavlopoulos J , Androutsopoulos I . Aspect Term Extraction for Sentiment Analysis: New Datasets, New Evaluation Measures and an Improved Unsupervised Method[C]// Workshop on Language Analysis for Social Media. 2014.
[32]	Bolshakova E , Loukachevitch N , Nokel M . Topic Models Can Improve Domain Term Extraction[J]. 2013.
[33]	Daille, Béatrice. Study and Implementation of Combined Techniques for Automatic Extraction of Terminology[C]// Balancing Act. 1996.
[34]	Gelbukh A F , Sidorov G , Lavin-Villa E , et al. Automatic Term Extraction Using Log-Likelihood Based Comparison with General Reference Corpus[J]. 2010.
[35]	Frantzi K T , Ananiadou S , Mima H . Automatic Recognition of Multi-word Terms: The C-value/ NC-value Method[C]// International Conference on Theory and Practice of Digital Libraries. 2000.
[36]	Piao, Scott & Forth, Jamie & Gacitua, Ricardo & Whittle, Jon & Wiggins, Geraint. (2010). Evaluating Tools for Automatic Concept Extraction: a Case Study from the Musicology Domain.
[37]	Justeson J S , Katz S M . Technical terminology: some linguistic properties and an algorithm for identification in text[J]. Natural Language Engineering, 1995, 1(01):9-27.
[38]	李丽双, 党延忠, 张婧, et al. 基于条件随机场的汽车领域术语抽取[J]. 大连理工大学学报, 2013, 53(002):267-272.
[39]	Li, L. & Dang, Y. & Zhang, Jihua & Li, D.. (2012). Domain term extraction based on conditional random fields combined with active learning strategy. 9. 1931-1940.
